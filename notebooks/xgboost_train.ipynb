{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Typing imports\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 32 # What batch file to use?\n",
    "EXCLUDE_AUXILIARY = False # Whether to exclude auxiliary pulses\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "# If either the event or time limit is reached the process will exit\n",
    "EVENT_LIMIT = 30\n",
    "TIME_LIMIT_HOURS = 1\n",
    "PULSE_AMOUNT = 200 # Amount of pulses to use for features\n",
    "TARGET_LABELS=['azimuth', 'zenith']\n",
    "VALIDATION_SPLIT=0.2 # Percentage of data to use for validation\n",
    "\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(filename='./artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_it_all(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        logging.info(f'Optimizing col {col}')\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    logging.info('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    logging.info('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def import_data(file: str):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    function_name = f\"read_{file.split('.')[-1]}\"\n",
    "    function = getattr(pd, function_name)\n",
    "    df = function(file)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def get_event_df(batch_df: pd.DataFrame, sensor_geometry: pd.DataFrame, event_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a DataFrame for a specific event.\n",
    "\n",
    "    Parameters:\n",
    "    train_batch_df (pandas.DataFrame): The batch DataFrame.\n",
    "    sensor_geometry (pandas.DataFrame): The sensor geometry DataFrame.\n",
    "    event_id (str): The event identifier.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing data for the specified event.\n",
    "    \"\"\"\n",
    "    if EXCLUDE_AUXILIARY:\n",
    "        batch_df = batch_df[~batch_df['auxiliary']].drop(columns=['auxiliary'])\n",
    "    \n",
    "    event_df = batch_df[batch_df['event_id'] == event_id]\n",
    "        \n",
    "    event_df = pd.merge(\n",
    "        left=event_df,\n",
    "        right=sensor_geometry,\n",
    "        how='left',\n",
    "        # blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "    \n",
    "    return event_df.drop(columns=['event_id', 'sensor_id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_batch_df = pd.read_parquet(f'{DATA_DIR}/{SET}/batch_1.parquet' ).reset_index()\n",
    "\n",
    "\n",
    "# test_batch_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_vector_shape(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Corrects the shape of the input vector.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe not sized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The newly sized dataframe that has the correct shape and filled with zeros.\n",
    "    \"\"\"\n",
    "    if len(df) < PULSE_AMOUNT:\n",
    "        \n",
    "        blank_df = pd.DataFrame(\n",
    "                index=range(len(df), PULSE_AMOUNT), columns=df.columns\n",
    "            ).fillna(0)\n",
    "        return pd.concat([df, blank_df], ignore_index=True)\n",
    "        \n",
    "    elif len(df) > PULSE_AMOUNT:\n",
    "        return df.head(PULSE_AMOUNT)\n",
    "        \n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(df: pd.DataFrame, event_id: int, is_training=False) -> pd.DataFrame:\n",
    "    \"\"\"Changes the rows of a dataframe to columns\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to be converted that currently has observations in rows\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single observation in columns\n",
    "    \"\"\"\n",
    "    df = make_input_vector_shape(df)\n",
    "    df = df.stack().reset_index()\n",
    "    df['features'] = df['level_0'].astype(str) + '_' + df['level_1']\n",
    "    df = df.drop(columns=['level_0','level_1']).set_index('features')\n",
    "    df = df.T.set_index(pd.Index([event_id]))\n",
    "    df.index.name = 'event_id'\n",
    "    \n",
    "    drop_cols = [ 'event_id']\n",
    "    \n",
    "    if is_training:\n",
    "        drop_cols.extend(['azimuth','zenith'])\n",
    "    \n",
    "    return pd.merge(\n",
    "        df, \n",
    "        meta_df[meta_df['event_id']== event_id][drop_cols], \n",
    "        on='event_id', \n",
    "        how='inner'\n",
    "    ).set_index('event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_geometry = import_data(f'{DATA_DIR}/sensor_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_df = get_event_df(test_batch_df, sensor_geometry, 24)\n",
    "# event_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_vec = get_input_vector(event_df, 24)\n",
    "# input_vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "def data_generator(\n",
    "    batch_paths: List[str],\n",
    "    # sequence_length: int,\n",
    "    # batch_size: int = BATCH_SIZE\n",
    "    is_training = False\n",
    "):\n",
    "\n",
    "    \"\"\"Emits a single event training example to be called by the model.fit_generator() method.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (List[str]): A list of paths to the batch files\n",
    "        sensor_geometry_df (pd.DataFrame): The sensor geometry dataframe\n",
    "        meta_df (pd.DataFrame): The dataframe containing the meta data\n",
    "        sequence_length (int): The length of the pulse sequence to use for training\n",
    "\n",
    "    Yields:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    batch_dtypes = {'event_id': 'int32', 'sensor_id': 'int16',\n",
    "                    'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'}\n",
    "    \n",
    "    av_batch_time_secs = None\n",
    "    av_event_time_secs = None\n",
    "    train_start_time = time.time()\n",
    "    events_processed = 0\n",
    "\n",
    "\n",
    "    for i, batch_path in enumerate(batch_paths):\n",
    "\n",
    "        batch_id = int(batch_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "        \n",
    "        logging.info(f'Processing batch {batch_id} of {len(batch_paths)}')\n",
    "        \n",
    "        output_df: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        batch_df = pd.read_parquet(batch_path).reset_index()\n",
    "            # type: ignore        \n",
    "        \n",
    "        # get the current date and time\n",
    "        now = datetime.datetime.now()\n",
    "        # create a date string with the format day-month-year-hour:minute\n",
    "        date_string = now.strftime('%d-%m-%Y-%H:%M')\n",
    "        # define the file path\n",
    "        file_path = f'artifacts/{SET}/{date_string}/batch_{batch_id}.csv'\n",
    "        parent_dir = os.path.dirname(file_path)\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "            \n",
    "        # Loop through unique event IDs\n",
    "        events = batch_df['event_id'].unique()\n",
    "        \n",
    "        for j, event_id in enumerate(events):\n",
    "            \n",
    "            logging.info(f'Processing event {event_id} of {len(events)} in batch {batch_id}')\n",
    "            \n",
    "            event_df = get_event_df(batch_df, sensor_geometry, event_id)\n",
    "            \n",
    "            input_vec =  get_input_vector(event_df, event_id, is_training)\n",
    "            \n",
    "            # check if a DataFrame exists\n",
    "            if output_df is not None:\n",
    "                \n",
    "                output_df = pd.concat([ output_df, input_vec])\n",
    "                input_vec.to_csv(file_path, mode='a', header=False, index=True)\n",
    "            else:\n",
    "                # handle the case where the DataFrame does not exist\n",
    "                output_df = input_vec\n",
    "                output_df.to_csv(file_path, index=True, index_label='event_id')\n",
    "            \n",
    "            \n",
    "            # Time tracking\n",
    "            current_time = time.time() - train_start_time\n",
    "            mins = current_time / 60\n",
    "            logging.info(f\"Total time taken so far: {round(mins, 2)} Minutes\")\n",
    "\n",
    "            av_event_time_secs = current_time if av_event_time_secs is None else (av_event_time_secs + current_time) / j + 1\n",
    "            \n",
    "            logging.info(f'Average event time: {round(av_event_time_secs, 2)} Seconds')\n",
    "\n",
    "            remaining_events = len(events) - j - 1\n",
    "            remaining_event_minutes = (av_event_time_secs * remaining_events)\n",
    "            \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Remaining Events to process for batch: {remaining_events}. \n",
    "                    Est time remaining to process: { round(remaining_event_minutes / 60 / 60, 2)} Hours\n",
    "                    \"\"\"\n",
    "                )\n",
    "            \n",
    "            \n",
    "            events_processed += 1\n",
    "            \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Total events processed so far: {events_processed}\n",
    "                \"\"\"\n",
    "                )\n",
    "            \n",
    "            if len(output_df) == BATCH_SIZE:\n",
    "                \n",
    "                yield output_df\n",
    "                \n",
    "                output_df = None\n",
    "                \n",
    "                \n",
    "        if batch_df is not None and False:\n",
    "        \n",
    "            file_path = f'artifacts/{SET}/{date_string}/{batch_id}.npy'\n",
    "            # create the parent directories if they don't exist\n",
    "            parent_dir = os.path.dirname(file_path)\n",
    "            \n",
    "            os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "            batch_df.to_numpy(file_path)\n",
    "            \n",
    "            current_time = time.time() - train_start_time\n",
    "            av_batch_time_secs = current_time if av_batch_time_secs is None else (av_batch_time_secs + current_time) / i + 1\n",
    "            \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Average batch time: {round(av_batch_time_secs / 60, 2)} Minutes\n",
    "                \"\"\"\n",
    "                )\n",
    "            \n",
    "            remaining_batches = len(events) - i - 1\n",
    "            remaining_batch_hours = (av_batch_time_secs * remaining_batches) / 60 / 60\n",
    "        \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Remaining Events to process for batch: {remaining_batches}, Est time remaining to process: {round(remaining_batch_hours, 2)} Hours\n",
    "                \"\"\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_directory = f'{DATA_DIR}/{SET}'\n",
    "batch_file_paths = [f'{batch_directory}/{file}' for file in os.listdir(batch_directory) if os.path.isfile(os.path.join(batch_directory, file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 132)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_split = int(len(batch_file_paths) * VALIDATION_SPLIT)\n",
    "\n",
    "training_batch_file_paths = batch_file_paths[:-file_split]\n",
    "validation_batch_file_paths = batch_file_paths[-file_split:]\n",
    "\n",
    "len(training_batch_file_paths), len(validation_batch_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object\n",
    "train_data_gen = data_generator(training_batch_file_paths, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = data_generator(validation_batch_file_paths, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Generator\n",
    "# next(train_data_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_test), len(y_test)\n",
    "# TIME_LIMIT_HOURS\n",
    "# X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/core.py:617: FutureWarning: Pass `objective` as keyword args.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# define a pipeline to preprocess the input and train the model\n",
    "\n",
    "params = {'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 100}\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', xgb.XGBRegressor(params))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch:  0  of  30  batches of  32  events.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[14:08:36] ../src/objective/objective.cc:26: Unknown objective function: `{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 100}`\nObjective candidate: survival:aft\nObjective candidate: binary:hinge\nObjective candidate: multi:softmax\nObjective candidate: multi:softprob\nObjective candidate: rank:pairwise\nObjective candidate: rank:ndcg\nObjective candidate: rank:map\nObjective candidate: reg:squarederror\nObjective candidate: reg:squaredlogerror\nObjective candidate: reg:logistic\nObjective candidate: binary:logistic\nObjective candidate: binary:logitraw\nObjective candidate: reg:linear\nObjective candidate: reg:pseudohubererror\nObjective candidate: count:poisson\nObjective candidate: survival:cox\nObjective candidate: reg:gamma\nObjective candidate: reg:tweedie\nObjective candidate: reg:absoluteerror\n\nStack trace:\n  [bt] (0) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x36cf3d) [0x7f5cab591f3d]\n  [bt] (1) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x36d629) [0x7f5cab592629]\n  [bt] (2) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x2daa02) [0x7f5cab4ffa02]\n  [bt] (3) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x2e4575) [0x7f5cab509575]\n  [bt] (4) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x2ddc29) [0x7f5cab502c29]\n  [bt] (5) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7f5cab352fb0]\n  [bt] (6) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/lib-dynload/../../libffi.so.8(+0xa052) [0x7f61ef798052]\n  [bt] (7) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/lib-dynload/../../libffi.so.8(+0x88cd) [0x7f61ef7968cd]\n  [bt] (8) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x2fe) [0x7f61efa22d6e]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train, y_train  \u001b[39m=\u001b[39m example\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mTARGET_LABELS), example[TARGET_LABELS]\n\u001b[1;32m      8\u001b[0m \u001b[39m# len(X_train), len(y_train),\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m# fit the pipeline on the training data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1026\u001b[0m     params,\n\u001b[1;32m   1027\u001b[0m     train_dmatrix,\n\u001b[1;32m   1028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [14:08:36] ../src/objective/objective.cc:26: Unknown objective function: `{'max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 100}`\nObjective candidate: survival:aft\nObjective candidate: binary:hinge\nObjective candidate: multi:softmax\nObjective candidate: multi:softprob\nObjective candidate: rank:pairwise\nObjective candidate: rank:ndcg\nObjective candidate: rank:map\nObjective candidate: reg:squarederror\nObjective candidate: reg:squaredlogerror\nObjective candidate: reg:logistic\nObjective candidate: binary:logistic\nObjective candidate: binary:logitraw\nObjective candidate: reg:linear\nObjective candidate: reg:pseudohubererror\nObjective candidate: count:poisson\nObjective candidate: survival:cox\nObjective candidate: reg:gamma\nObjective candidate: reg:tweedie\nObjective candidate: reg:absoluteerror\n\nStack trace:\n  [bt] (0) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x36cf3d) [0x7f5cab591f3d]\n  [bt] (1) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x36d629) [0x7f5cab592629]\n  [bt] (2) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x2daa02) [0x7f5cab4ffa02]\n  [bt] (3) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x2e4575) [0x7f5cab509575]\n  [bt] (4) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x2ddc29) [0x7f5cab502c29]\n  [bt] (5) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7f5cab352fb0]\n  [bt] (6) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/lib-dynload/../../libffi.so.8(+0xa052) [0x7f61ef798052]\n  [bt] (7) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/lib-dynload/../../libffi.so.8(+0x88cd) [0x7f61ef7968cd]\n  [bt] (8) /home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x2fe) [0x7f61efa22d6e]\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for step in range(EVENT_LIMIT):\n",
    "    \n",
    "    print('Training batch: ', step, ' of ', EVENT_LIMIT, ' batches of ', BATCH_SIZE, ' events.')\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    example = next(train_data_gen)\n",
    "    X_train, y_train  = example.drop(columns=TARGET_LABELS), example[TARGET_LABELS]\n",
    "    # len(X_train), len(y_train),\n",
    "\n",
    "    # fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation batch:  0  of  6  batches of  32  events.\n",
      "Accuracy:  -0.4 on validation set 0\n",
      "MSE: 2.7766727959153927\n",
      "Mean Absolute Error: 1.2594750854845769\n",
      "R2: -0.4009029284112363\n",
      "Validation batch:  1  of  6  batches of  32  events.\n",
      "Accuracy:  -0.48 on validation set 1\n",
      "MSE: 3.0231595757737066\n",
      "Mean Absolute Error: 1.2974820096864081\n",
      "R2: -0.4822985101708863\n",
      "Validation batch:  2  of  6  batches of  32  events.\n",
      "Accuracy:  -0.42 on validation set 2\n",
      "MSE: 3.053349252016077\n",
      "Mean Absolute Error: 1.3045031556225826\n",
      "R2: -0.4160809329580555\n",
      "Validation batch:  3  of  6  batches of  32  events.\n",
      "Accuracy:  -0.28 on validation set 3\n",
      "MSE: 2.5592359873781643\n",
      "Mean Absolute Error: 1.2190662324317112\n",
      "R2: -0.28171035135580524\n",
      "Validation batch:  4  of  6  batches of  32  events.\n",
      "Accuracy:  -0.25 on validation set 4\n",
      "MSE: 1.7423941673188166\n",
      "Mean Absolute Error: 1.0080166739294607\n",
      "R2: -0.25425004146911945\n",
      "Validation batch:  5  of  6  batches of  32  events.\n",
      "Accuracy:  -0.31 on validation set 5\n",
      "MSE: 2.8851564122856095\n",
      "Mean Absolute Error: 1.3020144778828484\n",
      "R2: -0.3101888271161719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "validation_batches = int(EVENT_LIMIT * VALIDATION_SPLIT)\n",
    "for step in range(validation_batches):\n",
    "    \n",
    "    print('Validation batch: ', step, ' of ', validation_batches, ' batches of ', BATCH_SIZE, ' events.')\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    example = next(val_data_gen)\n",
    "    X, y = example.drop(columns=TARGET_LABELS), example[TARGET_LABELS]\n",
    "    # len(X_train), len(y_train),\n",
    "\n",
    "    # fit the pipeline on the training data\n",
    "    score=pipeline.score(X, y)\n",
    "    print('Accuracy: ',round(score,2), 'on validation set', step)\n",
    "    # score the XGBoost model on the test set   \n",
    "    y_pred = pipeline.predict(X)\n",
    "    print('MSE:',  mean_squared_error(y, y_pred))\n",
    "    print('Mean Absolute Error:',  mean_absolute_error(y, y_pred))\n",
    "    print('R2:',  r2_score(y, y_pred))\n",
    "    print('R2:',  r2_score(y, y_pred))\n",
    "    \n",
    "    angular_dist_score()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = meta_df['batch_id'].unique().compute().values # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_hours = total_time / 60 / 60\n",
    "print(\"Total time taken: \", round(total_hours,2), \"Hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = [ angular_dist_score(y_test.iloc[i]['azimuth'], y_test.iloc[i]['zenith'], y_pred[i][0], y_pred[i][1]) for i in range(len(y_test))] \n",
    "print(f'Accuracy: {sum(accuracy) / len(accuracy)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
