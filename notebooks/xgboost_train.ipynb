{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Typing imports\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 32 # What batch file to use?\n",
    "EXCLUDE_AUXILIARY = False # Whether to exclude auxiliary pulses\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "# If either the event or time limit is reached the process will exit\n",
    "EVENT_LIMIT = 30\n",
    "TIME_LIMIT_HOURS = 1\n",
    "PULSE_AMOUNT = 200 # Amount of pulses to use for features\n",
    "TARGET_LABELS=['azimuth', 'zenith']\n",
    "\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(filename='./artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_it_all(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        logging.info(f'Optimizing col {col}')\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    logging.info('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    logging.info('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def import_data(file: str):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    function_name = f\"read_{file.split('.')[-1]}\"\n",
    "    function = getattr(pd, function_name)\n",
    "    df = function(file)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def get_event_df(batch_df: pd.DataFrame, sensor_geometry: pd.DataFrame, event_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a DataFrame for a specific event.\n",
    "\n",
    "    Parameters:\n",
    "    train_batch_df (pandas.DataFrame): The batch DataFrame.\n",
    "    sensor_geometry (pandas.DataFrame): The sensor geometry DataFrame.\n",
    "    event_id (str): The event identifier.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing data for the specified event.\n",
    "    \"\"\"\n",
    "    if EXCLUDE_AUXILIARY:\n",
    "        batch_df = batch_df[~batch_df['auxiliary']].drop(columns=['auxiliary'])\n",
    "    \n",
    "    event_df = batch_df[batch_df['event_id'] == event_id]\n",
    "        \n",
    "    event_df = pd.merge(\n",
    "        left=event_df,\n",
    "        right=sensor_geometry,\n",
    "        how='left',\n",
    "        # blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "    \n",
    "    return event_df.drop(columns=['event_id', 'sensor_id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_batch_df = pd.read_parquet(f'{DATA_DIR}/{SET}/batch_1.parquet' ).reset_index()\n",
    "\n",
    "\n",
    "# test_batch_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_vector_shape(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Corrects the shape of the input vector.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe not sized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The newly sized dataframe that has the correct shape and filled with zeros.\n",
    "    \"\"\"\n",
    "    if len(df) < PULSE_AMOUNT:\n",
    "        \n",
    "        blank_df = pd.DataFrame(\n",
    "                index=range(len(df), PULSE_AMOUNT), columns=df.columns\n",
    "            ).fillna(0)\n",
    "        return pd.concat([df, blank_df], ignore_index=True)\n",
    "        \n",
    "    elif len(df) > PULSE_AMOUNT:\n",
    "        return df.head(PULSE_AMOUNT)\n",
    "        \n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(df: pd.DataFrame, event_id: int, is_training=False) -> pd.DataFrame:\n",
    "    \"\"\"Changes the rows of a dataframe to columns\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to be converted that currently has observations in rows\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single observation in columns\n",
    "    \"\"\"\n",
    "    df = make_input_vector_shape(df)\n",
    "    df = df.stack().reset_index()\n",
    "    df['features'] = df['level_0'].astype(str) + '_' + df['level_1']\n",
    "    df = df.drop(columns=['level_0','level_1']).set_index('features')\n",
    "    df = df.T.set_index(pd.Index([event_id]))\n",
    "    df.index.name = 'event_id'\n",
    "    \n",
    "    drop_cols = [ 'event_id']\n",
    "    \n",
    "    if is_training:\n",
    "        drop_cols.extend(['azimuth','zenith'])\n",
    "    \n",
    "    return pd.merge(\n",
    "        df, \n",
    "        meta_df[meta_df['event_id']== event_id][drop_cols], \n",
    "        on='event_id', \n",
    "        how='inner'\n",
    "    ).set_index('event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_geometry = import_data(f'{DATA_DIR}/sensor_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_df = get_event_df(test_batch_df, sensor_geometry, 24)\n",
    "# event_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_vec = get_input_vector(event_df, 24)\n",
    "# input_vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "def data_generator(\n",
    "    batch_paths: List[str],\n",
    "    # sequence_length: int,\n",
    "    # batch_size: int = BATCH_SIZE\n",
    "    is_training = False\n",
    "):\n",
    "\n",
    "    \"\"\"Emits a single event training example to be called by the model.fit_generator() method.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (List[str]): A list of paths to the batch files\n",
    "        sensor_geometry_df (pd.DataFrame): The sensor geometry dataframe\n",
    "        meta_df (pd.DataFrame): The dataframe containing the meta data\n",
    "        sequence_length (int): The length of the pulse sequence to use for training\n",
    "\n",
    "    Yields:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    batch_dtypes = {'event_id': 'int32', 'sensor_id': 'int16',\n",
    "                    'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'}\n",
    "    \n",
    "    av_batch_time_secs = None\n",
    "    av_event_time_secs = None\n",
    "    train_start_time = time.time()\n",
    "    events_processed = 0\n",
    "\n",
    "\n",
    "    for i, batch_path in enumerate(batch_paths):\n",
    "\n",
    "        batch_id = int(batch_path.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "        \n",
    "        logging.info(f'Processing batch {batch_id} of {len(batch_paths)}')\n",
    "        \n",
    "        output_df: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        batch_df = pd.read_parquet(batch_path).reset_index()\n",
    "            # type: ignore        \n",
    "        \n",
    "        # get the current date and time\n",
    "        now = datetime.datetime.now()\n",
    "        # create a date string with the format day-month-year-hour:minute\n",
    "        date_string = now.strftime('%d-%m-%Y-%H:%M')\n",
    "        # define the file path\n",
    "        file_path = f'artifacts/{SET}/{date_string}/batch_{batch_id}.csv'\n",
    "        parent_dir = os.path.dirname(file_path)\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "            \n",
    "        # Loop through unique event IDs\n",
    "        events = batch_df['event_id'].unique()\n",
    "        \n",
    "        for j, event_id in enumerate(events):\n",
    "            \n",
    "            logging.info(f'Processing event {event_id} of {len(events)} in batch {batch_id}')\n",
    "            \n",
    "            event_df = get_event_df(batch_df, sensor_geometry, event_id)\n",
    "            \n",
    "            input_vec =  get_input_vector(event_df, event_id, is_training)\n",
    "            \n",
    "            # check if a DataFrame exists\n",
    "            if output_df is not None:\n",
    "                \n",
    "                output_df = pd.concat([ output_df, input_vec])\n",
    "                input_vec.to_csv(file_path, mode='a', header=False, index=True)\n",
    "            else:\n",
    "                # handle the case where the DataFrame does not exist\n",
    "                output_df = input_vec\n",
    "                output_df.to_csv(file_path, index=True, index_label='event_id')\n",
    "            \n",
    "            \n",
    "            # Time tracking\n",
    "            current_time = time.time() - train_start_time\n",
    "            mins = current_time / 60\n",
    "            logging.info(f\"Total time taken so far: {round(mins, 2)} Minutes\")\n",
    "\n",
    "            av_event_time_secs = current_time if av_event_time_secs is None else (av_event_time_secs + current_time) / j + 1\n",
    "            \n",
    "            logging.info(f'Average event time: {round(av_event_time_secs, 2)} Seconds')\n",
    "\n",
    "            remaining_events = len(events) - j - 1\n",
    "            remaining_event_minutes = (av_event_time_secs * remaining_events)\n",
    "            \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Remaining Events to process for batch: {remaining_events}. \n",
    "                    Est time remaining to process: { round(remaining_event_minutes / 60 / 60, 2)} Hours\n",
    "                    \"\"\"\n",
    "                )\n",
    "            \n",
    "            \n",
    "            events_processed += 1\n",
    "            \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Total events processed so far: {events_processed}\n",
    "                \"\"\"\n",
    "                )\n",
    "            \n",
    "            if len(output_df) == BATCH_SIZE:\n",
    "                \n",
    "                yield output_df\n",
    "                \n",
    "                output_df = None\n",
    "                \n",
    "                \n",
    "        if batch_df is not None and False:\n",
    "        \n",
    "            file_path = f'artifacts/{SET}/{date_string}/{batch_id}.npy'\n",
    "            # create the parent directories if they don't exist\n",
    "            parent_dir = os.path.dirname(file_path)\n",
    "            \n",
    "            os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "            batch_df.to_numpy(file_path)\n",
    "            \n",
    "            current_time = time.time() - train_start_time\n",
    "            av_batch_time_secs = current_time if av_batch_time_secs is None else (av_batch_time_secs + current_time) / i + 1\n",
    "            \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Average batch time: {round(av_batch_time_secs / 60, 2)} Minutes\n",
    "                \"\"\"\n",
    "                )\n",
    "            \n",
    "            remaining_batches = len(events) - i - 1\n",
    "            remaining_batch_hours = (av_batch_time_secs * remaining_batches) / 60 / 60\n",
    "        \n",
    "            logging.info(\n",
    "                f\"\"\"\n",
    "                    Remaining Events to process for batch: {remaining_batches}, Est time remaining to process: {round(remaining_batch_hours, 2)} Hours\n",
    "                \"\"\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_directory = f'{DATA_DIR}/{SET}'\n",
    "batch_file_paths = [f'{batch_directory}/{file}' for file in os.listdir(batch_directory) if os.path.isfile(os.path.join(batch_directory, file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_file_paths = batch_file_paths[:-1]\n",
    "validation_batch_file_paths = batch_file_paths[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object\n",
    "train_data_gen = data_generator(training_batch_file_paths, True)\n",
    "val_data_gen = data_generator(validation_batch_file_paths, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Generator\n",
    "# next(train_data_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_test), len(y_test)\n",
    "# TIME_LIMIT_HOURS\n",
    "# X_test, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pipeline to preprocess the input and train the model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', xgb.XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch:  0  of  30  batches of  32  events.\n",
      "Training batch:  1  of  30  batches of  32  events.\n",
      "Training batch:  2  of  30  batches of  32  events.\n",
      "Training batch:  3  of  30  batches of  32  events.\n",
      "Training batch:  4  of  30  batches of  32  events.\n",
      "Training batch:  5  of  30  batches of  32  events.\n",
      "Training batch:  6  of  30  batches of  32  events.\n",
      "Training batch:  7  of  30  batches of  32  events.\n",
      "Training batch:  8  of  30  batches of  32  events.\n",
      "Training batch:  9  of  30  batches of  32  events.\n",
      "Training batch:  10  of  30  batches of  32  events.\n",
      "Training batch:  11  of  30  batches of  32  events.\n",
      "Training batch:  12  of  30  batches of  32  events.\n",
      "Training batch:  13  of  30  batches of  32  events.\n",
      "Training batch:  14  of  30  batches of  32  events.\n",
      "Training batch:  15  of  30  batches of  32  events.\n",
      "Training batch:  16  of  30  batches of  32  events.\n",
      "Training batch:  17  of  30  batches of  32  events.\n",
      "Training batch:  18  of  30  batches of  32  events.\n",
      "Training batch:  19  of  30  batches of  32  events.\n",
      "Training batch:  20  of  30  batches of  32  events.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for step in range(EVENT_LIMIT):\n",
    "    \n",
    "    print('Training batch: ', step, ' of ', EVENT_LIMIT, ' batches of ', BATCH_SIZE, ' events.')\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    example = next(train_data_gen)\n",
    "    X_train, y_train  = example.drop(columns=TARGET_LABELS), example[TARGET_LABELS]\n",
    "    # len(X_train), len(y_train),\n",
    "\n",
    "    # fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step in range(EVENT_LIMIT):\n",
    "    \n",
    "    print('Validation batch: ', step, ' of ', EVENT_LIMIT, ' batches of ', BATCH_SIZE, ' events.')\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    example = next(val_data_gen)\n",
    "    X, y = example.drop(columns=TARGET_LABELS), example[TARGET_LABELS]\n",
    "    # len(X_train), len(y_train),\n",
    "\n",
    "    # fit the pipeline on the training data\n",
    "    pipeline.score(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = meta_df['batch_id'].unique().compute().values # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_hours = total_time / 60 / 60\n",
    "print(\"Total time taken: \", round(total_hours,2), \"Hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = [ angular_dist_score(y_test.iloc[i]['azimuth'], y_test.iloc[i]['zenith'], y_pred[i][0], y_pred[i][1]) for i in range(len(y_test))] \n",
    "print(f'Accuracy: {sum(accuracy) / len(accuracy)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "av_batch_time_secs = None\n",
    "av_event_time_secs = None\n",
    "train_start_time = time.time()\n",
    "events_processed = 0\n",
    "\n",
    "batches = meta_df['batch_id'].unique().compute().values\n",
    "\n",
    "for i, batch_id in enumerate(batches):\n",
    "    \n",
    "    logging.info(f'Processing batch {batch_id} of {len(batches)}')\n",
    "    \n",
    "    # The batch dataframe to be populated with events\n",
    "    batch_df = None\n",
    "    \n",
    "    batch_dfd = dd.read_parquet(f'{DATA_DIR}/{SET}/batch_{batch_id}.parquet', \n",
    "        blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "    \n",
    "    # get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "    # create a date string with the format day-month-year-hour:minute\n",
    "    date_string = now.strftime('%d-%m-%Y-%H:%M')\n",
    "    # define the file path\n",
    "    file_path = f'artifacts/{SET}/{date_string}/batch_{batch_id}.csv'\n",
    "    parent_dir = os.path.dirname(file_path)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "        \n",
    "    # Loop through unique event IDs\n",
    "    events = batch_dfd['event_id'].unique().compute().values\n",
    "    \n",
    "    for j, event_id in enumerate(events):\n",
    "        \n",
    "        logging.info(f'Processing event {event_id} of {len(events)} in batch {batch_id}')\n",
    "        \n",
    "        event_df = get_event_df(batch_dfd, sensor_geometry, event_id)\n",
    "        \n",
    "        input_vec =  get_input_vector(event_df, event_id)\n",
    "        \n",
    "        # check if a DataFrame exists\n",
    "        if batch_df is not None:\n",
    "            \n",
    "            batch_df = pd.concat([ batch_df, input_vec])\n",
    "            input_vec.to_csv(file_path, mode='a', header=False, index=True)\n",
    "        else:\n",
    "            # handle the case where the DataFrame does not exist\n",
    "            batch_df = input_vec\n",
    "            batch_df.to_csv(file_path, index=True, index_label='event_id')\n",
    "         \n",
    "        \n",
    "        # Time tracking\n",
    "        current_time = time.time() - train_start_time\n",
    "        mins = current_time / 60\n",
    "        logging.info(f\"Total time taken so far: {round(mins, 2)} Minutes\")\n",
    "\n",
    "        av_event_time_secs = current_time if av_event_time_secs is None else (av_event_time_secs + current_time) / j + 1\n",
    "        \n",
    "        logging.info(f'Average event time: {round(av_event_time_secs, 2)} Seconds')\n",
    "\n",
    "        remaining_events = len(events) - j - 1\n",
    "        remaining_event_minutes = (av_event_time_secs * remaining_events)\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Remaining Events to process for batch: {remaining_events}. \n",
    "                Est time remaining to process: { round(remaining_event_minutes / 60 / 60, 2)} Hours\n",
    "                \"\"\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        events_processed += 1\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Total events processed so far: {events_processed}\n",
    "            \"\"\"\n",
    "            )\n",
    "            \n",
    "    if batch_df is not None:\n",
    "       \n",
    "        file_path = f'artifacts/{SET}/{date_string}/{batch_id}.npy'\n",
    "        # create the parent directories if they don't exist\n",
    "        parent_dir = os.path.dirname(file_path)\n",
    "        \n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "        batch_df.to_numpy(file_path)\n",
    "        \n",
    "        current_time = time.time() - train_start_time\n",
    "        av_batch_time_secs = current_time if av_batch_time_secs is None else (av_batch_time_secs + current_time) / i + 1\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Average batch time: {round(av_batch_time_secs / 60, 2)} Minutes\n",
    "            \"\"\"\n",
    "            )\n",
    "        \n",
    "        remaining_batches = len(events) - i - 1\n",
    "        remaining_batch_hours = (av_batch_time_secs * remaining_batches) / 60 / 60\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Remaining Events to process for batch: {remaining_batches}, Est time remaining to process: {round(remaining_batch_hours, 2)} Hours\n",
    "            \"\"\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
