{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe import DataFrame as DaskDataframe, read_parquet as dask_read_parquet\n",
    "import glob\n",
    "\n",
    "\n",
    "# Typing imports\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# BATCH = 1 # What batch file to use?\n",
    "EXCLUDE_AUXILIARY = True # Whether to exclude auxiliary pulses\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "# If either the event or time limit is reached the process will exit\n",
    "EVENT_LIMIT = 3000\n",
    "TIME_LIMIT_HOURS = 1\n",
    "PULSE_AMOUNT = 200 # Amount of pulses to use for features\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(filename='artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_it_all(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        logging.info(f'Optimizing col {col}')\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    logging.info('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    logging.info('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def import_data(file: str):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    function_name = f\"read_{file.split('.')[-1]}\"\n",
    "    function = getattr(pd, function_name)\n",
    "    df = function(file)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "# def get_event_df(batch_df: dd.DataFrame, sensor_geometry: pd.DataFrame, event_id: int) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Get a DataFrame for a specific event.\n",
    "\n",
    "#     Parameters:\n",
    "#     train_batch_df (pandas.DataFrame): The batch DataFrame.\n",
    "#     sensor_geometry (pandas.DataFrame): The sensor geometry DataFrame.\n",
    "#     event_id (str): The event identifier.\n",
    "\n",
    "#     Returns:\n",
    "#     pandas.DataFrame: A DataFrame containing data for the specified event.\n",
    "#     \"\"\"\n",
    "#     if EXCLUDE_AUXILIARY:\n",
    "#         batch_df = batch_df[~batch_df['auxiliary']].drop(columns=['auxiliary'])\n",
    "    \n",
    "#     event_df = batch_df[batch_df['event_id'] == event_id].compute()\n",
    "        \n",
    "#     event_df = pd.merge(\n",
    "#         left=event_df,\n",
    "#         right=sensor_geometry,\n",
    "#         how='left',\n",
    "#         blocksize=64000000 # = 64 Mb chunks,\n",
    "#     ).reset_index()\n",
    "#     return event_df.drop(columns=['event_id', 'sensor_id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = dd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet', \n",
    "    blocksize=64000000 # = 64 Mb chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>time</th>\n",
       "      <th>charge</th>\n",
       "      <th>auxiliary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1754501072</th>\n",
       "      <td>2445</td>\n",
       "      <td>6019</td>\n",
       "      <td>0.975</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sensor_id  time  charge  auxiliary\n",
       "event_id                                      \n",
       "1754501072       2445  6019   0.975       True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the pattern to match files\n",
    "pattern = f\"{DATA_DIR}/{SET}/*.parquet\"\n",
    "\n",
    "# Get a list of all files that match the pattern\n",
    "batch_file_paths = glob.glob(pattern)\n",
    "\n",
    "# Load the files into a list of Dask dataframes\n",
    "dfs = [dd.read_parquet(file, assume_missing=True) for file in batch_file_paths]\n",
    "\n",
    "# Concatenate the dataframes into one long table\n",
    "batches_df = dd.concat(dfs)\n",
    "\n",
    "batches_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_df = batches_df[~batches_df['auxiliary']].drop(columns=['auxiliary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>time</th>\n",
       "      <th>charge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1754501072</th>\n",
       "      <td>4572</td>\n",
       "      <td>9892</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754501072</th>\n",
       "      <td>4573</td>\n",
       "      <td>9933</td>\n",
       "      <td>1.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754501072</th>\n",
       "      <td>4573</td>\n",
       "      <td>9940</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754501072</th>\n",
       "      <td>4573</td>\n",
       "      <td>9988</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754501072</th>\n",
       "      <td>4574</td>\n",
       "      <td>10518</td>\n",
       "      <td>1.225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sensor_id   time  charge\n",
       "event_id                            \n",
       "1754501072       4572   9892   0.675\n",
       "1754501072       4573   9933   1.175\n",
       "1754501072       4573   9940   0.375\n",
       "1754501072       4573   9988   0.225\n",
       "1754501072       4574  10518   1.225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>time</th>\n",
       "      <th>charge</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1754501072</td>\n",
       "      <td>4572</td>\n",
       "      <td>9892</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-101.06</td>\n",
       "      <td>490.22</td>\n",
       "      <td>297.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_id  sensor_id  time  charge       x       y       z\n",
       "0  1754501072       4572  9892   0.675 -101.06  490.22  297.24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sensor_geometry = import_data(f'{DATA_DIR}/sensor_geometry.csv')\n",
    "sensor_geometry_df = dd.read_csv(f\"{DATA_DIR}/sensor_geometry.csv\", assume_missing=True)\n",
    "\n",
    "event_df = batches_df.reset_index().merge(\n",
    "        sensor_geometry_df,\n",
    "        how='inner',\n",
    "        on='sensor_id'\n",
    "    )\n",
    "event_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_vector_shape(df: DaskDataframe) -> pd.DataFrame:\n",
    "    \"\"\"Corrects the shape of the input vector.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe not sized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The newly sized dataframe that has the correct shape and filled with zeros.\n",
    "    \"\"\"\n",
    "    if len(df) < PULSE_AMOUNT:\n",
    "        \n",
    "        blank_df = pd.DataFrame(\n",
    "                index=range(len(df), PULSE_AMOUNT), columns=df.columns\n",
    "            ).fillna(0)\n",
    "        return pd.concat([df, blank_df], ignore_index=True)\n",
    "        \n",
    "    elif len(df) > PULSE_AMOUNT:\n",
    "        return df.head(PULSE_AMOUNT)\n",
    "        \n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(df: DaskDataframe, event_id: int) -> DaskDataframe:\n",
    "    \"\"\"Changes the rows of a dataframe to columns\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to be converted that currently has observations in rows\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single observation in columns\n",
    "    \"\"\"\n",
    "    df = make_input_vector_shape(df)\n",
    "    df = df.stack().reset_index()\n",
    "    df['features'] = df['level_0'].astype(str) + '_' + df['level_1']\n",
    "    df = df.drop(columns=['level_0','level_1']).set_index('features')\n",
    "    df = df.T.set_index(pd.Index([event_id]))\n",
    "    df.index.name = 'event_id'\n",
    "    \n",
    "    return pd.merge(\n",
    "        df, \n",
    "        meta_dfd[meta_dfd['event_id']== event_id].compute()[[ 'event_id','azimuth','zenith' ]], \n",
    "        on='event_id', \n",
    "        how='inner'\n",
    "    ).set_index('event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DaskDataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_event_df\u001b[39m(batch_df: DaskDataFrame, sensor_geometry: DaskDataFrame, event_id: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    Get a DataFrame for a specific event.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m    pandas.DataFrame: A DataFrame containing data for the specified event.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m EXCLUDE_AUXILIARY:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DaskDataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_event_df(batch_df: DaskDataFrame, sensor_geometry: DaskDataFrame, event_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a DataFrame for a specific event.\n",
    "\n",
    "    Parameters:\n",
    "    train_batch_df (pandas.DataFrame): The batch DataFrame.\n",
    "    sensor_geometry (pandas.DataFrame): The sensor geometry DataFrame.\n",
    "    event_id (str): The event identifier.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing data for the specified event.\n",
    "    \"\"\"\n",
    "    if EXCLUDE_AUXILIARY:\n",
    "        batch_df = batch_df[~batch_df['auxiliary']].drop(columns=['auxiliary'])\n",
    "    \n",
    "    event_df = batch_df[batch_df['event_id'] == event_id].compute()\n",
    "        \n",
    "    event_df = pd.merge(\n",
    "        left=event_df,\n",
    "        right=sensor_geometry,\n",
    "        how='inner',\n",
    "        on='sensor_id'\n",
    "    ),\n",
    "    \n",
    "    ## Drop columns that are not needed for prediction\n",
    "    return event_df.drop(columns=['event_id', 'sensor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = get_event_df(test_batch_dfd, sensor_geometry, 24)\n",
    "event_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_input_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_vec \u001b[39m=\u001b[39m get_input_vector(event_df, \u001b[39m24\u001b[39m)\n\u001b[1;32m      2\u001b[0m input_vec\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_input_vector' is not defined"
     ]
    }
   ],
   "source": [
    "input_vec = get_input_vector(event_df, 24)\n",
    "input_vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m av_batch_time_secs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m av_event_time_secs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m events_processed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      8\u001b[0m batches \u001b[39m=\u001b[39m meta_dfd[\u001b[39m'\u001b[39m\u001b[39mbatch_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mcompute()\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "av_batch_time_secs = None\n",
    "av_event_time_secs = None\n",
    "train_start_time = time.time()\n",
    "events_processed = 0\n",
    "\n",
    "batches = meta_dfd['batch_id'].unique().compute().values\n",
    "\n",
    "for i, batch_id in enumerate(batches):\n",
    "    \n",
    "    logging.info(f'Processing batch {batch_id} of {len(batches)}')\n",
    "    \n",
    "    # The batch dataframe to be populated with events\n",
    "    batch_df = None\n",
    "    \n",
    "    batch_dfd = dd.read_parquet(f'{DATA_DIR}/{SET}/batch_{batch_id}.parquet', \n",
    "        blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "    \n",
    "    # get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "    # create a date string with the format day-month-year-hour:minute\n",
    "    date_string = now.strftime('%d-%m-%Y-%H:%M')\n",
    "    # define the file path\n",
    "    file_path = f'artifacts/{SET}/{date_string}/batch_{batch_id}.csv'\n",
    "    parent_dir = os.path.dirname(file_path)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "        \n",
    "    # Loop through unique event IDs\n",
    "    events = batch_dfd['event_id'].unique().compute().values\n",
    "    \n",
    "    for j, event_id in enumerate(events):\n",
    "        \n",
    "        logging.info(f'Processing event {event_id} of {len(events)} in batch {batch_id}')\n",
    "        \n",
    "        event_df = get_event_df(batch_dfd, sensor_geometry, event_id)\n",
    "        \n",
    "        input_vec =  get_input_vector(event_df, event_id)\n",
    "        \n",
    "        # check if a DataFrame exists\n",
    "        if batch_df is not None:\n",
    "            \n",
    "            batch_df = pd.concat([ batch_df, input_vec])\n",
    "            input_vec.to_csv(file_path, mode='a', header=False, index=True)\n",
    "        else:\n",
    "            # handle the case where the DataFrame does not exist\n",
    "            batch_df = input_vec\n",
    "            batch_df.to_csv(file_path, index=True, index_label='event_id')\n",
    "         \n",
    "        \n",
    "        # Time tracking\n",
    "        current_time = time.time() - train_start_time\n",
    "        mins = current_time / 60\n",
    "        logging.info(f\"Total time taken so far: {round(mins, 2)} Minutes\")\n",
    "\n",
    "        av_event_time_secs = current_time if av_event_time_secs is None else (av_event_time_secs + current_time) / j + 1\n",
    "        \n",
    "        logging.info(f'Average event time: {round(av_event_time_secs, 2)} Seconds')\n",
    "\n",
    "        remaining_events = len(events) - j - 1\n",
    "        remaining_event_minutes = (av_event_time_secs * remaining_events)\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Remaining Events to process for batch: {remaining_events}. \n",
    "                Est time remaining to process: { round(remaining_event_minutes / 60 / 60, 2)} Hours\n",
    "                \"\"\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        events_processed += 1\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Total events processed so far: {events_processed}\n",
    "            \"\"\"\n",
    "            )\n",
    "            \n",
    "    if batch_df is not None:\n",
    "       \n",
    "        file_path = f'artifacts/{SET}/{date_string}/{batch_id}.npy'\n",
    "        # create the parent directories if they don't exist\n",
    "        parent_dir = os.path.dirname(file_path)\n",
    "        \n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "        batch_df.to_numpy(file_path)\n",
    "        \n",
    "        current_time = time.time() - train_start_time\n",
    "        av_batch_time_secs = current_time if av_batch_time_secs is None else (av_batch_time_secs + current_time) / i + 1\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Average batch time: {round(av_batch_time_secs / 60, 2)} Minutes\n",
    "            \"\"\"\n",
    "            )\n",
    "        \n",
    "        remaining_batches = len(events) - i - 1\n",
    "        remaining_batch_hours = (av_batch_time_secs * remaining_batches) / 60 / 60\n",
    "        \n",
    "        logging.info(\n",
    "            f\"\"\"\n",
    "                Remaining Events to process for batch: {remaining_batches}, Est time remaining to process: {round(remaining_batch_hours, 2)} Hours\n",
    "            \"\"\"\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# load the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39martifacts/train/16-02-2023-20:49/batch_1.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(training_file_path)\n\u001b[1;32m      5\u001b[0m targets\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mazimuth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mzenith\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[39m# split the data into training and testing sets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the data\n",
    "training_file_path = 'artifacts/train/16-02-2023-20:49/batch_1.csv'\n",
    "data = pd.read_csv(training_file_path)\n",
    "\n",
    "targets=['azimuth', 'zenith']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(targets, axis=1), data[targets], test_size=0.2, random_state=42)\n",
    "\n",
    "# define a pipeline to preprocess the input and train the model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', xgb.XGBClassifier())\n",
    "])\n",
    "\n",
    "# fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_dfd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_ids \u001b[39m=\u001b[39m meta_dfd[\u001b[39m'\u001b[39m\u001b[39mbatch_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39mcompute()\u001b[39m.\u001b[39mvalues \u001b[39m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta_dfd' is not defined"
     ]
    }
   ],
   "source": [
    "batch_ids = meta_dfd['batch_id'].unique().compute().values # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m total_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n\u001b[1;32m      3\u001b[0m total_hours \u001b[39m=\u001b[39m total_time \u001b[39m/\u001b[39m \u001b[39m60\u001b[39m \u001b[39m/\u001b[39m \u001b[39m60\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_hours = total_time / 60 / 60\n",
    "print(\"Total time taken: \", round(total_hours,2), \"Hours\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
