{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Neutrino Direction with an Convolutional neural network\n",
    "\n",
    "Using a Tensorflow LSTM layer using the event time steps to the input to predict the Neutrino Direction azimuth and zenith angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "from sys import getsizeof\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('..')\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Typing imports\n",
    "from typing import List\n",
    "\n",
    "from scripts.utils import seed_it_all, convert_bytes_to_gmbkb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MODEL_TYPE='lstm' # Which model to use\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "SEED=10\n",
    "\n",
    "TRY_TO_USE_GPU=True # Whether to try to use GPU\n",
    "\n",
    "EPOCHS=20\n",
    "STEPS_PER_EPOCH=32\n",
    "PULSE_AMOUNT = 100 # Amount of pulses to use for features\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "FEATURES = [ 'time', 'charge', 'auxiliary', 'x', 'y', 'z'] # Which features to use as the model input\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "Num GPUs Available:  1\n",
      "CUDA version:  True\n",
      "cuDNN version:  True\n",
      "TensorFlow version:  2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 18:45:23.985565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-27 18:45:23.985732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-27 18:45:23.985824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-27 18:45:23.985965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-27 18:45:23.986067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-27 18:45:23.986129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 7632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/aj/anaconda3/envs/KAG_IC_NEU:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "# packages in environment at /home/aj/anaconda3/envs/KAG_IC_NEU:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n"
     ]
    }
   ],
   "source": [
    "using_gpu = False\n",
    "\n",
    "if not TRY_TO_USE_GPU:\n",
    "  os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "else:\n",
    "  device_name = tf.test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    print('GPU device NOT found')\n",
    "  else:\n",
    "    using_gpu=True\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    print(\"CUDA version: \", tf.test.is_built_with_cuda())\n",
    "    print(\"cuDNN version: \", tf.test.is_built_with_gpu_support())\n",
    "    print(\"TensorFlow version: \", tf.__version__)\n",
    "    \n",
    "    !conda list cudatoolkit\n",
    "    !conda list cudnn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_it_all(SEED)\n",
    "# set the seed for the random number generator\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-256.25</td>\n",
       "      <td>-521.0</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_id       x      y      z\n",
       "0          0 -256.25 -521.0  496.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_dtypes = { 'x': 'float16', 'y': 'float16', 'z': 'float16' }\n",
    "sensor_geometry_df = pd.read_csv(f'{DATA_DIR}/sensor_geometry.csv', dtype=sensor_dtypes) # type: ignore\n",
    "sensor_geometry_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70.69 KB'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(sensor_geometry_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>first_pulse_index</th>\n",
       "      <th>last_pulse_index</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>5.03125</td>\n",
       "      <td>2.087891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id  event_id  first_pulse_index  last_pulse_index  azimuth    zenith\n",
       "0         1        24                  0                60  5.03125  2.087891"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "meta_dtypes = {'batch_id': 'int16', 'event_id': 'Int64', 'first_pulse_index': 'int32', 'last_pulse_index': 'int32', 'azimuth': 'float16', 'zenith': 'float16'}\n",
    "meta_df = pd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet').astype(meta_dtypes)\n",
    "meta_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_directory = f'{DATA_DIR}/{SET}'\n",
    "batch_file_paths = [f'{batch_directory}/{file}' for file in os.listdir(batch_directory) if os.path.isfile(os.path.join(batch_directory, file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_file_paths = batch_file_paths[:-1]\n",
    "validation_batch_file_paths = batch_file_paths[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch_df= pd.read_parquet(batch_file_paths[1])\n",
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_input_observation(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame,) -> np.ndarray: \n",
    "    \"\"\"Gets a single event input observation for the model\n",
    "\n",
    "    Args:\n",
    "        batch (pd.DataFrame): The batch dataframe\n",
    "        event_id (int): The event id to find within the batch df\n",
    "        sequence_length (int): The length of the sequence to use\n",
    "        sensor_geometry (pd.DataFrame): The sensor geometry dataframe\n",
    "\n",
    "    Returns:\n",
    "        np.array: A single event input observation for the model\n",
    "    \"\"\"\n",
    "    # The event dataframe with a list of pulse readings\n",
    "    event_data = batch[batch['event_id'] == event_id]\n",
    "    \n",
    "    merged_df = pd.merge(event_data, sensor_geometry, on='sensor_id', how='left')\n",
    "    \n",
    "    # get the first N pulses with N being the sequence length\n",
    "    sequence = merged_df.head(sequence_length)[FEATURES]\n",
    "    n_missing = PULSE_AMOUNT - len(sequence)\n",
    "    if n_missing > 0:\n",
    "        df_missing = pd.DataFrame(0, index=np.arange(n_missing), columns=sequence.columns)\n",
    "        sequence = pd.concat([sequence, df_missing])\n",
    "        \n",
    "    return sequence.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_data(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame, meta_data: pd.DataFrame):\n",
    "    \n",
    "    input_sequence = get_event_input_observation(batch, event_id, sequence_length, sensor_geometry)\n",
    "    \n",
    "    # get the target labels \n",
    "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0] \n",
    "    \n",
    "    # reshape the sequence and target labels to be fed into the model\n",
    "    return np.reshape(input_sequence, (1, sequence_length, len(FEATURES))), np.reshape(target_labels, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(\n",
    "    batch_paths: List[str],\n",
    "    sensor_geometry: pd.DataFrame,\n",
    "    meta_data: pd.DataFrame,\n",
    "    sequence_length: int,\n",
    "    batch_size: int = BATCH_SIZE\n",
    "):\n",
    "    \"\"\"Emits a single event training example to be called by the model.fit_generator() method.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (List[str]): A list of paths to the batch files\n",
    "        sensor_geometry_df (pd.DataFrame): The sensor geometry dataframe\n",
    "        meta_df (pd.DataFrame): The dataframe containing the meta data\n",
    "        sequence_length (int): The length of the pulse sequence to use for training\n",
    "\n",
    "    Yields:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    batch_dtypes = {'event_id': 'int32', 'sensor_id': 'int16',\n",
    "                    'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'}\n",
    "\n",
    "    for batch_path in batch_paths:\n",
    "\n",
    "        batch = pd.read_parquet(batch_path).reset_index().astype(batch_dtypes)\n",
    "\n",
    "        output_batch_x = None\n",
    "        output_batch_y = None\n",
    "\n",
    "        for event_id in batch['event_id'].unique():\n",
    "\n",
    "            x_batch, y_batch = get_event_data(\n",
    "                batch, event_id, sequence_length, sensor_geometry, meta_data)\n",
    "\n",
    "            x_tensor = tf.constant(x_batch)\n",
    "            y_tensor = tf.constant(y_batch)\n",
    "\n",
    "            if output_batch_x is None and output_batch_y is None:\n",
    "                output_batch_x = x_tensor\n",
    "                output_batch_y = y_tensor\n",
    "                logging.debug('Output_batch initializing')\n",
    "\n",
    "            else:\n",
    "                output_batch_x = tf.concat([output_batch_x, x_tensor], axis=0)\n",
    "                output_batch_y = tf.concat([output_batch_y, y_tensor], axis=0)\n",
    "\n",
    "                logging.debug('Output_batch extending: %s',\n",
    "                              len(output_batch_x))\n",
    "\n",
    "            if len(output_batch_x) == batch_size:\n",
    "                output = output_batch_x[:], output_batch_y[:]\n",
    "                output_batch_x = None\n",
    "                output_batch_y = None\n",
    "                yield output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object\n",
    "train_data_gen = data_generator(training_batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT, batch_size=BATCH_SIZE)\n",
    "val_data_gen = data_generator(validation_batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = f'{MODEL_TYPE}_{datetime.now().strftime(\"%d%m%Y%H%M%S\")}'.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"checkpoints/{RUN_ID}/{'epoch:02d'}\",\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch'),\n",
    "\n",
    "    ModelCheckpoint(f'checkpoints/{RUN_ID}/best_model_weights.h5',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True,\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    verbose=1),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10\n",
    "    ),\n",
    "\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=1e-8 # type: ignore\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CuDNNLSTM\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "lstm_type = CuDNNLSTM if using_gpu else LSTM\n",
    "# lstm_type = LSTM\n",
    "\n",
    "print('Using', lstm_type.__name__)\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE), \n",
    "\n",
    "# model.add(LSTM(LSTM_UNITS,input_shape=(PULSE_AMOUNT, len(FEATURES))))\n",
    "model.add(lstm_type(LSTM_UNITS,input_shape=(PULSE_AMOUNT, len(FEATURES))))\n",
    "model.add(Dense(2, activation='linear')) # set the number of output neurons to 2 and the activation function to linear\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='mean_squared_error', \n",
    "    # optimizer=optimizer, \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "The amount of training examples that are in an epoch when using is not the entire dataset like in normal problems. \n",
    "Because the total number of training examples are not known the amount of training examples in an epoch is calculated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.tensorflow.autolog(every_n_iter=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial lstm_27022023184542: LSTM model using 120 sequence length with 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 19:35:21.769194: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2023-02-27 19:35:21.769212: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2023-02-27 19:35:21.769239: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2023-02-27 19:35:21.769251: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2023-02-27 19:35:21.769265: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1/32 [..............................] - ETA: 22s - loss: 9.1378 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 19:35:22.526590: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2023-02-27 19:35:22.526617: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2023-02-27 19:35:22.526650: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/32 [>.............................] - ETA: 1:59 - loss: 8.5146 - accuracy: 0.4844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 19:35:26.509498: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2023-02-27 19:35:26.509545: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1757] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2023-02-27 19:35:26.511013: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2023-02-27 19:35:26.511824: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2023-02-27 19:35:26.514706: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26\n",
      "\n",
      "2023-02-27 19:35:26.516650: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.trace.json.gz\n",
      "2023-02-27 19:35:26.523528: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26\n",
      "\n",
      "2023-02-27 19:35:26.525626: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.memory_profile.json.gz\n",
      "2023-02-27 19:35:26.525779: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26\n",
      "Dumped tool data for xplane.pb to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/tmppfno82wc/train/plugins/profile/2023_02_27_19_35_26/aj-ubuntu-2004-3.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/32 [========================>.....] - ETA: 20s - loss: 3.9798 - accuracy: 0.7060"
     ]
    }
   ],
   "source": [
    "\n",
    "description = f'LSTM model using {PULSE_AMOUNT} sequence length with {LSTM_UNITS}'\n",
    "print(f'--- Starting trial {RUN_ID}: {description}')\n",
    "\n",
    "mlflow.set_experiment('LSTM')\n",
    "mlflow.start_run(run_name=RUN_ID, description=description)\n",
    "mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "mlflow.log_param('sequence_length', PULSE_AMOUNT)\n",
    "mlflow.log_param('lstm_units', LSTM_UNITS)\n",
    "\n",
    "# mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "# mlflow.log_param('epochs')\n",
    "history=None\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_data_gen,\n",
    "        steps_per_epoch=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        \n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        # use_multiprocessing=True,\n",
    "        # workers=4,\n",
    "        \n",
    "        # Validation Settings\n",
    "        validation_data=val_data_gen, \n",
    "        validation_batch_size=BATCH_SIZE,\n",
    "        validation_steps=3, \n",
    "        validation_freq=1,\n",
    "        \n",
    "    )\n",
    "    mlflow.end_run()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    mlflow.end_run(status='FAILED')\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
