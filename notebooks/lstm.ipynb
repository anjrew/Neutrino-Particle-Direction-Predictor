{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Neutrino Direction with an LSTM\n",
    "\n",
    "Using a Tensorflow LSTM layer using the event time steps to the input to predict the Neutrino Direction azimuth and zenith angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 19:08:29.290911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 19:08:29.367184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:29.367197: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-23 19:08:29.796706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:29.796747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:29.796752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "from sys import getsizeof\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('..')\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Typing imports\n",
    "from typing import List, Tuple\n",
    "\n",
    "from scripts.utils import seed_it_all, convert_bytes_to_gmbkb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device NOT found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 19:08:30.372774: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 19:08:30.394232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:08:30.394359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394481: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394511: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394565: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394625: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-02-23 19:08:30.394630: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device NOT found')\n",
    "else:\n",
    "  print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MODEL_TYPE='lstm' # Which model to use\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "SEED=10\n",
    "\n",
    "# TIME_LIMIT_HOURS = 1\n",
    "\n",
    "LSTM_UNITS = 64\n",
    "EPOCHS=10\n",
    "STEPS_PER_EPOCH=100\n",
    "PULSE_AMOUNT = 100 # Amount of pulses to use for features\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "FEATURES = [ 'time', 'charge', 'auxiliary', 'x', 'y', 'z'] # Which features to use as the model input\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 19:08:30.649408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-23 19:08:30.649525: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_it_all(SEED)\n",
    "# set the seed for the random number generator\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-256.25</td>\n",
       "      <td>-521.0</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_id       x      y      z\n",
       "0          0 -256.25 -521.0  496.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_dtypes = { 'x': 'float16', 'y': 'float16', 'z': 'float16' }\n",
    "sensor_geometry_df = pd.read_csv(f'{DATA_DIR}/sensor_geometry.csv', dtype=sensor_dtypes) # type: ignore\n",
    "sensor_geometry_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70.69 KB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(sensor_geometry_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>first_pulse_index</th>\n",
       "      <th>last_pulse_index</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>5.03125</td>\n",
       "      <td>2.087891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id  event_id  first_pulse_index  last_pulse_index  azimuth    zenith\n",
       "0         1        24                  0                60  5.03125  2.087891"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "meta_dtypes = {'batch_id': 'int16', 'event_id': 'Int64', 'first_pulse_index': 'int32', 'last_pulse_index': 'int32', 'azimuth': 'float16', 'zenith': 'float16'}\n",
    "meta_df = pd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet').astype(meta_dtypes)\n",
    "meta_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch file path 3 Samples:\n"
     ]
    }
   ],
   "source": [
    "batch_directory = f'{DATA_DIR}/{SET}'\n",
    "batch_file_paths = [f'{batch_directory}/{file}' for file in os.listdir(batch_directory) if os.path.isfile(os.path.join(batch_directory, file))]\n",
    "print('First batch file path 3 Samples:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_file_paths = batch_file_paths[:-1]\n",
    "validation_batch_file_paths = batch_file_paths[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch_df= pd.read_parquet(batch_file_paths[1])\n",
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_input_observation(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame,) -> np.array: \n",
    "    \"\"\"Gets a single event input observation for the model\n",
    "\n",
    "    Args:\n",
    "        batch (pd.DataFrame): The batch dataframe\n",
    "        event_id (int): The event id to find within the batch df\n",
    "        sequence_length (int): The length of the sequence to use\n",
    "        sensor_geometry (pd.DataFrame): The sensor geometry dataframe\n",
    "\n",
    "    Returns:\n",
    "        np.array: A single event input observation for the model\n",
    "    \"\"\"\n",
    "    # The event dataframe with a list of pulse readings\n",
    "    event_data = batch[batch['event_id'] == event_id]\n",
    "    \n",
    "    merged_df = pd.merge(event_data, sensor_geometry, on='sensor_id', how='left')\n",
    "    \n",
    "    # get the first N pulses with N being the sequence length\n",
    "    sequence = merged_df.head(sequence_length)[FEATURES]\n",
    "    n_missing = 100 - len(sequence)\n",
    "    if n_missing > 0:\n",
    "        df_missing = pd.DataFrame(0, index=np.arange(n_missing), columns=sequence.columns)\n",
    "        sequence = pd.concat([sequence, df_missing])\n",
    "        \n",
    "    return sequence.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_data(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame, meta_data: pd.DataFrame):\n",
    "    \n",
    "    input_sequence = get_event_input_observation(batch, event_id, sequence_length, sensor_geometry)\n",
    "    \n",
    "    # get the target labels \n",
    "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0] \n",
    "    \n",
    "    # reshape the sequence and target labels to be fed into the model\n",
    "    return np.reshape(input_sequence, (1, sequence_length, len(FEATURES))), np.reshape(target_labels, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(\n",
    "    batch_paths: List[str],\n",
    "    sensor_geometry: pd.DataFrame,\n",
    "    meta_data: pd.DataFrame,\n",
    "    sequence_length: int,\n",
    "    batch_size: int = BATCH_SIZE\n",
    "):\n",
    "    \"\"\"Emits a single event training example to be called by the model.fit_generator() method.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (List[str]): A list of paths to the batch files\n",
    "        sensor_geometry_df (pd.DataFrame): The sensor geometry dataframe\n",
    "        meta_df (pd.DataFrame): The dataframe containing the meta data\n",
    "        sequence_length (int): The length of the pulse sequence to use for training\n",
    "\n",
    "    Yields:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    batch_dtypes = {'event_id': 'int32', 'sensor_id': 'int16',\n",
    "                    'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'}\n",
    "\n",
    "    for batch_path in batch_paths:\n",
    "\n",
    "        batch = pd.read_parquet(batch_path).reset_index().astype(batch_dtypes)\n",
    "\n",
    "        output_batch_x = None\n",
    "        output_batch_y = None\n",
    "\n",
    "        for event_id in batch['event_id'].unique():\n",
    "\n",
    "            x_batch, y_batch = get_event_data(\n",
    "                batch, event_id, sequence_length, sensor_geometry, meta_data)\n",
    "\n",
    "            x_tensor = tf.constant(x_batch)\n",
    "            y_tensor = tf.constant(y_batch)\n",
    "\n",
    "            if output_batch_x is None and output_batch_y is None:\n",
    "                output_batch_x = x_tensor\n",
    "                output_batch_y = y_tensor\n",
    "                logging.debug('Output_batch initializing')\n",
    "\n",
    "            else:\n",
    "                output_batch_x = tf.concat([output_batch_x, x_tensor], axis=0)\n",
    "                output_batch_y = tf.concat([output_batch_y, y_tensor], axis=0)\n",
    "\n",
    "                logging.debug('Output_batch extending: %s',\n",
    "                              len(output_batch_x))\n",
    "\n",
    "            if len(output_batch_x) == batch_size:\n",
    "                output = output_batch_x[:], output_batch_y[:]\n",
    "                output_batch_x = None\n",
    "                output_batch_y = None\n",
    "                yield output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object\n",
    "train_data_gen = data_generator(training_batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT)\n",
    "val_data_gen = data_generator(validation_batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = f'{MODEL_TYPE}_{datetime.now().strftime(\"%d%m%Y%H%M%S\")}'.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define the callback to save checkpoints\n",
    "callbacks = [\n",
    "\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"checkpoints/{RUN_ID}/{'epoch:02d'}\",\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch'),\n",
    "\n",
    "    ModelCheckpoint(f'checkpoints/{RUN_ID}/best_model_weights.h5',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True,\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    verbose=1),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10\n",
    "    ),\n",
    "\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=1e-8 # type: ignore\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(LSTM_UNITS,input_shape=(PULSE_AMOUNT, len(FEATURES))))\n",
    "model.add(Dense(2, activation='linear')) # set the number of output neurons to 2 and the activation function to linear\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.tensorflow.autolog(every_n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: lstm_23022023190838\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.9536 - accuracy: 0.6453WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "100/100 [==============================] - 188s 2s/step - loss: 2.9536 - accuracy: 0.6453 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - ETA: 0s - loss: 2.1564 - accuracy: 0.7281\n",
      "Epoch 2: val_loss improved from inf to 2.13189, saving model to checkpoints/lstm_23022023190838/best_model_weights.h5\n",
      "100/100 [==============================] - 274s 3s/step - loss: 2.1564 - accuracy: 0.7281 - val_loss: 2.1319 - val_accuracy: 0.7412 - lr: 0.0010\n",
      "Epoch 3/10\n",
      " 59/100 [================>.............] - ETA: 1:13 - loss: 2.1070 - accuracy: 0.7537"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-6:\n",
      "Process Keras_worker_ForkPoolWorker-7:\n",
      "Process Keras_worker_ForkPoolWorker-8:\n",
      "Process Keras_worker_ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "Process Keras_worker_ForkPoolWorker-1:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 866, in next_sample\n",
      "    return next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py\", line 1181, in __restore_generator\n",
      "    yield from prev_generator\n",
      "  File \"/tmp/ipykernel_5277/2290958631.py\", line 33, in data_generator\n",
      "    x_batch, y_batch = get_event_data(\n",
      "  File \"/tmp/ipykernel_5277/569224257.py\", line 6, in get_event_data\n",
      "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0]\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 72, in new_method\n",
      "    return method(self, other)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 42, in __eq__\n",
      "    return self._cmp_method(other, operator.eq)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/series.py\", line 6243, in _cmp_method\n",
      "    res_values = ops.comparison_op(lvalues, rvalues, op)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\", line 273, in comparison_op\n",
      "    res_values = op(lvalues, rvalues)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 72, in new_method\n",
      "    return method(self, other)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 42, in __eq__\n",
      "    return self._cmp_method(other, operator.eq)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arrays/masked.py\", line 718, in _cmp_method\n",
      "    result = method(other)\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 866, in next_sample\n",
      "    return next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py\", line 1181, in __restore_generator\n",
      "    yield from prev_generator\n",
      "  File \"/tmp/ipykernel_5277/2290958631.py\", line 33, in data_generator\n",
      "    x_batch, y_batch = get_event_data(\n",
      "  File \"/tmp/ipykernel_5277/569224257.py\", line 6, in get_event_data\n",
      "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0]\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/frame.py\", line 3798, in __getitem__\n",
      "    return self._getitem_bool_array(key)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/frame.py\", line 3852, in _getitem_bool_array\n",
      "    indexer = key.nonzero()[0]\n",
      "KeyboardInterrupt\n",
      "Process Keras_worker_ForkPoolWorker-2:\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process Keras_worker_ForkPoolWorker-3:\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 866, in next_sample\n",
      "    return next(_SHARED_SEQUENCES[uid])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py\", line 1181, in __restore_generator\n",
      "    yield from prev_generator\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/tmp/ipykernel_5277/2290958631.py\", line 33, in data_generator\n",
      "    x_batch, y_batch = get_event_data(\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_5277/569224257.py\", line 6, in get_event_data\n",
      "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0]\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 72, in new_method\n",
      "    return method(self, other)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/keras/utils/data_utils.py\", line 866, in next_sample\n",
      "    return next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 42, in __eq__\n",
      "    return self._cmp_method(other, operator.eq)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py\", line 1181, in __restore_generator\n",
      "    yield from prev_generator\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/series.py\", line 6243, in _cmp_method\n",
      "    res_values = ops.comparison_op(lvalues, rvalues, op)\n",
      "  File \"/tmp/ipykernel_5277/2290958631.py\", line 33, in data_generator\n",
      "    x_batch, y_batch = get_event_data(\n",
      "  File \"/tmp/ipykernel_5277/569224257.py\", line 6, in get_event_data\n",
      "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0]\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\", line 273, in comparison_op\n",
      "    res_values = op(lvalues, rvalues)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 72, in new_method\n",
      "    return method(self, other)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 72, in new_method\n",
      "    return method(self, other)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 42, in __eq__\n",
      "    return self._cmp_method(other, operator.eq)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 42, in __eq__\n",
      "    return self._cmp_method(other, operator.eq)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/series.py\", line 6243, in _cmp_method\n",
      "    res_values = ops.comparison_op(lvalues, rvalues, op)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arrays/masked.py\", line 718, in _cmp_method\n",
      "    result = method(other)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\", line 273, in comparison_op\n",
      "    res_values = op(lvalues, rvalues)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/ops/common.py\", line 72, in new_method\n",
      "    return method(self, other)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arraylike.py\", line 42, in __eq__\n",
      "    return self._cmp_method(other, operator.eq)\n",
      "  File \"/home/aj/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/core/arrays/masked.py\", line 718, in _cmp_method\n",
      "    result = method(other)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('--- Starting trial: %s' % RUN_ID)\n",
    "\n",
    "mlflow.set_experiment('LSTM')\n",
    "mlflow.start_run(run_name=RUN_ID, description=f'LSTM model using {PULSE_AMOUNT} sequence length with {LSTM_UNITS}')\n",
    "mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "mlflow.log_param('sequence_length', PULSE_AMOUNT)\n",
    "mlflow.log_param('lstm_units', LSTM_UNITS)\n",
    "\n",
    "# mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "# mlflow.log_param('epochs')\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "    validation_data=val_data_gen, validation_steps=100, epochs=100,\n",
    "    \n",
    "    # steps_per_epoch=100\n",
    "    \n",
    "    \n",
    "    # epochs=EPOCHS, \n",
    "    # batch_size=BATCH_SIZE,\n",
    "    # callbacks=callbacks,\n",
    "    # validation_data=val_data_gen,\n",
    "    # validation_steps=math.ceil(STEPS_PER_EPOCH/4),\n",
    "    # validation_batch_size=math.ceil(STEPS_PER_EPOCH/4),\n",
    "    # validation_freq=2,\n",
    "    # use_multiprocessing=True,\n",
    "    # workers=4,\n",
    "    # verbose='1'\n",
    ")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate(test_sequences, test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
