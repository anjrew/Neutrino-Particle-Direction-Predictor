{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Neutrino Direction with an LSTM\n",
    "\n",
    "Using a Tensorflow LSTM layer using the event time steps to the input to predict the Neutrino Direction azimuth and zenith angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Third-party library imports\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Typing imports\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[39m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m is_numpy_dev \u001b[39mas\u001b[39;00m _is_numpy_dev  \u001b[39m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_libs\u001b[39;00m \u001b[39mimport\u001b[39;00m hashtable \u001b[39mas\u001b[39;00m _hashtable, lib \u001b[39mas\u001b[39;00m _lib, tslib \u001b[39mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_typing\u001b[39;00m \u001b[39mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyarrow\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:23\u001b[0m\n\u001b[1;32m     19\u001b[0m     np_percentile_argname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minterpolation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m _nlv \u001b[39m<\u001b[39m Version(_min_numpy_ver):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthis version of pandas is incompatible with numpy < \u001b[39m\u001b[39m{\u001b[39;00m_min_numpy_ver\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myour numpy version is \u001b[39m\u001b[39m{\u001b[39;00m_np_version\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease upgrade numpy to >= \u001b[39m\u001b[39m{\u001b[39;00m_min_numpy_ver\u001b[39m}\u001b[39;00m\u001b[39m to use this pandas version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     30\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m_np_version\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_numpy_dev\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "from sys import getsizeof\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('..')\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Typing imports\n",
    "from typing import List\n",
    "\n",
    "from scripts.utils import seed_it_all, convert_bytes_to_gmbkb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MODEL_TYPE='lstm' # Which model to use\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "SEED=10\n",
    "\n",
    "TRY_TO_USE_GPU=True # Whether to try to use GPU\n",
    "\n",
    "LSTM_UNITS = 64\n",
    "EPOCHS=20\n",
    "STEPS_PER_EPOCH=50\n",
    "PULSE_AMOUNT = 120 # Amount of pulses to use for features\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "FEATURES = [ 'time', 'charge', 'auxiliary', 'x', 'y', 'z'] # Which features to use as the model input\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m   os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m   device_name \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtest\u001b[39m.\u001b[39mgpu_device_name()\n\u001b[1;32m      6\u001b[0m   \u001b[39mif\u001b[39;00m device_name \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/device:GPU:0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGPU device NOT found\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "if not TRY_TO_USE_GPU:\n",
    "  os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "else:\n",
    "  device_name = tf.test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    print('GPU device NOT found')\n",
    "  else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    print(\"CUDA version: \", tf.test.is_built_with_cuda())\n",
    "    print(\"cuDNN version: \", tf.test.is_built_with_gpu_support())\n",
    "    print(\"TensorFlow version: \", tf.__version__)\n",
    "    \n",
    "    !conda list cudatoolkit\n",
    "    !conda list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "CUDA version:  True\n",
      "cuDNN version:  True\n",
      "TensorFlow version:  2.11.0\n",
      "/bin/bash: /home/aj/anaconda3/envs/KAG_IC_NEU/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "# packages in environment at /home/aj/anaconda3/envs/KAG_IC_NEU:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "cudatoolkit               11.2.2              hbe64b41_10    conda-forge\n",
      "/bin/bash: /home/aj/anaconda3/envs/KAG_IC_NEU/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "# packages in environment at /home/aj/anaconda3/envs/KAG_IC_NEU:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "cudnn                     8.1.0.77             h90431f1_0    conda-forge\n",
      "nvidia-cudnn              8.2.0.51                 pypi_0    pypi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 11:15:18.146375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 11:15:18.146865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 11:15:18.147250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 11:15:18.147889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 11:15:18.148378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 11:15:18.148818: I tensorflow/core/common_runtime/gpu/gpu"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_device.cc:1613] Created device /device:GPU:0 with 759 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_it_all(SEED)\n",
    "# set the seed for the random number generator\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-256.25</td>\n",
       "      <td>-521.0</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_id       x      y      z\n",
       "0          0 -256.25 -521.0  496.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_dtypes = { 'x': 'float16', 'y': 'float16', 'z': 'float16' }\n",
    "sensor_geometry_df = pd.read_csv(f'{DATA_DIR}/sensor_geometry.csv', dtype=sensor_dtypes) # type: ignore\n",
    "sensor_geometry_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70.69 KB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(sensor_geometry_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>first_pulse_index</th>\n",
       "      <th>last_pulse_index</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>5.03125</td>\n",
       "      <td>2.087891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id  event_id  first_pulse_index  last_pulse_index  azimuth    zenith\n",
       "0         1        24                  0                60  5.03125  2.087891"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "meta_dtypes = {'batch_id': 'int16', 'event_id': 'Int64', 'first_pulse_index': 'int32', 'last_pulse_index': 'int32', 'azimuth': 'float16', 'zenith': 'float16'}\n",
    "meta_df = pd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet').astype(meta_dtypes)\n",
    "meta_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch file path 3 Samples:\n"
     ]
    }
   ],
   "source": [
    "batch_directory = f'{DATA_DIR}/{SET}'\n",
    "batch_file_paths = [f'{batch_directory}/{file}' for file in os.listdir(batch_directory) if os.path.isfile(os.path.join(batch_directory, file))]\n",
    "print('First batch file path 3 Samples:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_file_paths = batch_file_paths[:-1]\n",
    "validation_batch_file_paths = batch_file_paths[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch_df= pd.read_parquet(batch_file_paths[1])\n",
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_input_observation(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame,) -> np.ndarray: \n",
    "    \"\"\"Gets a single event input observation for the model\n",
    "\n",
    "    Args:\n",
    "        batch (pd.DataFrame): The batch dataframe\n",
    "        event_id (int): The event id to find within the batch df\n",
    "        sequence_length (int): The length of the sequence to use\n",
    "        sensor_geometry (pd.DataFrame): The sensor geometry dataframe\n",
    "\n",
    "    Returns:\n",
    "        np.array: A single event input observation for the model\n",
    "    \"\"\"\n",
    "    # The event dataframe with a list of pulse readings\n",
    "    event_data = batch[batch['event_id'] == event_id]\n",
    "    \n",
    "    merged_df = pd.merge(event_data, sensor_geometry, on='sensor_id', how='left')\n",
    "    \n",
    "    # get the first N pulses with N being the sequence length\n",
    "    sequence = merged_df.head(sequence_length)[FEATURES]\n",
    "    n_missing = PULSE_AMOUNT - len(sequence)\n",
    "    if n_missing > 0:\n",
    "        df_missing = pd.DataFrame(0, index=np.arange(n_missing), columns=sequence.columns)\n",
    "        sequence = pd.concat([sequence, df_missing])\n",
    "        \n",
    "    return sequence.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_data(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame, meta_data: pd.DataFrame):\n",
    "    \n",
    "    input_sequence = get_event_input_observation(batch, event_id, sequence_length, sensor_geometry)\n",
    "    \n",
    "    # get the target labels \n",
    "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0] \n",
    "    \n",
    "    # reshape the sequence and target labels to be fed into the model\n",
    "    return np.reshape(input_sequence, (1, sequence_length, len(FEATURES))), np.reshape(target_labels, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(\n",
    "    batch_paths: List[str],\n",
    "    sensor_geometry: pd.DataFrame,\n",
    "    meta_data: pd.DataFrame,\n",
    "    sequence_length: int,\n",
    "    batch_size: int = BATCH_SIZE\n",
    "):\n",
    "    \"\"\"Emits a single event training example to be called by the model.fit_generator() method.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (List[str]): A list of paths to the batch files\n",
    "        sensor_geometry_df (pd.DataFrame): The sensor geometry dataframe\n",
    "        meta_df (pd.DataFrame): The dataframe containing the meta data\n",
    "        sequence_length (int): The length of the pulse sequence to use for training\n",
    "\n",
    "    Yields:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    batch_dtypes = {'event_id': 'int32', 'sensor_id': 'int16',\n",
    "                    'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'}\n",
    "\n",
    "    for batch_path in batch_paths:\n",
    "\n",
    "        batch = pd.read_parquet(batch_path).reset_index().astype(batch_dtypes)\n",
    "\n",
    "        output_batch_x = None\n",
    "        output_batch_y = None\n",
    "\n",
    "        for event_id in batch['event_id'].unique():\n",
    "\n",
    "            x_batch, y_batch = get_event_data(\n",
    "                batch, event_id, sequence_length, sensor_geometry, meta_data)\n",
    "\n",
    "            x_tensor = tf.constant(x_batch)\n",
    "            y_tensor = tf.constant(y_batch)\n",
    "\n",
    "            if output_batch_x is None and output_batch_y is None:\n",
    "                output_batch_x = x_tensor\n",
    "                output_batch_y = y_tensor\n",
    "                logging.debug('Output_batch initializing')\n",
    "\n",
    "            else:\n",
    "                output_batch_x = tf.concat([output_batch_x, x_tensor], axis=0)\n",
    "                output_batch_y = tf.concat([output_batch_y, y_tensor], axis=0)\n",
    "\n",
    "                logging.debug('Output_batch extending: %s',\n",
    "                              len(output_batch_x))\n",
    "\n",
    "            if len(output_batch_x) == batch_size:\n",
    "                output = output_batch_x[:], output_batch_y[:]\n",
    "                output_batch_x = None\n",
    "                output_batch_y = None\n",
    "                yield output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object\n",
    "train_data_gen = data_generator(training_batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT, batch_size=BATCH_SIZE)\n",
    "val_data_gen = data_generator(validation_batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = f'{MODEL_TYPE}_{datetime.now().strftime(\"%d%m%Y%H%M%S\")}'.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"checkpoints/{RUN_ID}/{'epoch:02d'}\",\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch'),\n",
    "\n",
    "    ModelCheckpoint(f'checkpoints/{RUN_ID}/best_model_weights.h5',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=True,\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    verbose=1),\n",
    "\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=10\n",
    "    ),\n",
    "\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=1e-8 # type: ignore\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 10:50:13.250399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 10:50:13.250631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 10:50:13.250759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 10:50:13.250973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 10:50:13.251162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-24 10:50:13.251256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 759 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(LSTM_UNITS,input_shape=(PULSE_AMOUNT, len(FEATURES))))\n",
    "# model.add(CuDNNLSTM(LSTM_UNITS,input_shape=(PULSE_AMOUNT, len(FEATURES))))\n",
    "model.add(Dense(2, activation='linear')) # set the number of output neurons to 2 and the activation function to linear\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "The amount of training examples that are in an epoch when using is not the entire dataset like in normal problems. \n",
    "Because the total number of training examples are not known the amount of training examples in an epoch is calculated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.tensorflow.autolog(every_n_iter=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial lstm_24022023105013: LSTM model using 120 sequence length with 64\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 10:50:21.075954: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-02-24 10:50:21.075992: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1554 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph execution error:\n",
      "\n",
      "Fail to find the dnn implementation.\n",
      "\t [[{{node CudnnRNN}}]]\n",
      "\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_3188]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_3188]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m     34\u001b[0m mlflow\u001b[39m.\u001b[39mend_run(status\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFAILED\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m history\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     15\u001b[0m         train_data_gen,\n\u001b[1;32m     16\u001b[0m         steps_per_epoch\u001b[39m=\u001b[39;49mBATCH_SIZE, \n\u001b[1;32m     17\u001b[0m         epochs\u001b[39m=\u001b[39;49mEPOCHS, \n\u001b[1;32m     18\u001b[0m         \n\u001b[1;32m     19\u001b[0m         batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     20\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     21\u001b[0m         \u001b[39m# use_multiprocessing=True,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m         \u001b[39m# workers=4,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m         \n\u001b[1;32m     24\u001b[0m         \u001b[39m# Validation Settings\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m         validation_data\u001b[39m=\u001b[39;49mval_data_gen, \n\u001b[1;32m     26\u001b[0m         validation_batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     27\u001b[0m         validation_steps\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, \n\u001b[1;32m     28\u001b[0m         validation_freq\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m         \n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run()\n\u001b[1;32m     32\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:553\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m try_log_autologging_event(\n\u001b[1;32m    544\u001b[0m     AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_patch_function_start,\n\u001b[1;32m    545\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     kwargs,\n\u001b[1;32m    550\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m patch_is_class:\n\u001b[0;32m--> 553\u001b[0m     patch_function\u001b[39m.\u001b[39;49mcall(call_original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     patch_function(call_original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[0;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mcls\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:181\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_exception(e)\n\u001b[1;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:174\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, original, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    173\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_patch_implementation(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:232\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mlflow\u001b[39m.\u001b[39mactive_run():\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run \u001b[39m=\u001b[39m create_managed_run()\n\u001b[0;32m--> 232\u001b[0m result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_patch_implementation(original, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanaged_run:\n\u001b[1;32m    235\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run(RunStatus\u001b[39m.\u001b[39mto_string(RunStatus\u001b[39m.\u001b[39mFINISHED))\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/tensorflow/__init__.py:1226\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[0;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m early_stop_callback \u001b[39m=\u001b[39m _get_early_stop_callback(callbacks)\n\u001b[1;32m   1224\u001b[0m _log_early_stop_callback_params(early_stop_callback)\n\u001b[0;32m-> 1226\u001b[0m history \u001b[39m=\u001b[39m original(inst, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m log_models:\n\u001b[1;32m   1229\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:536\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39m_og_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_og_kwargs)\n\u001b[1;32m    534\u001b[0m         \u001b[39mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 536\u001b[0m \u001b[39mreturn\u001b[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:471\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    464\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    465\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         og_kwargs,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 471\u001b[0m     original_fn_result \u001b[39m=\u001b[39m original_fn(\u001b[39m*\u001b[39;49mog_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mog_kwargs)\n\u001b[1;32m    473\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    474\u001b[0m         AutologgingEventLogger\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    475\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m         og_kwargs,\n\u001b[1;32m    480\u001b[0m     )\n\u001b[1;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py:533\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    530\u001b[0m     disable_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m     reroute_warnings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    532\u001b[0m ):\n\u001b[0;32m--> 533\u001b[0m     original_result \u001b[39m=\u001b[39m original(\u001b[39m*\u001b[39;49m_og_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_og_kwargs)\n\u001b[1;32m    534\u001b[0m     \u001b[39mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/KAG_IC_NEU/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_3188]"
     ]
    }
   ],
   "source": [
    "\n",
    "description = f'LSTM model using {PULSE_AMOUNT} sequence length with {LSTM_UNITS}'\n",
    "print(f'--- Starting trial {RUN_ID}: {description}')\n",
    "\n",
    "mlflow.set_experiment('LSTM')\n",
    "mlflow.start_run(run_name=RUN_ID, description=description)\n",
    "mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "mlflow.log_param('sequence_length', PULSE_AMOUNT)\n",
    "mlflow.log_param('lstm_units', LSTM_UNITS)\n",
    "\n",
    "# mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "# mlflow.log_param('epochs')\n",
    "history=None\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_data_gen,\n",
    "        steps_per_epoch=BATCH_SIZE, \n",
    "        epochs=EPOCHS, \n",
    "        \n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        # use_multiprocessing=True,\n",
    "        # workers=4,\n",
    "        \n",
    "        # Validation Settings\n",
    "        validation_data=val_data_gen, \n",
    "        validation_batch_size=BATCH_SIZE,\n",
    "        validation_steps=3, \n",
    "        validation_freq=1,\n",
    "        \n",
    "    )\n",
    "    mlflow.end_run()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    mlflow.end_run(status='FAILED')\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlflow\u001b[39m.\u001b[39mend_run()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# loss = model.evaluate(test_sequences, test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
