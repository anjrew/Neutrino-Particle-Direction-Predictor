{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Neutrino Direction with an LSTM\n",
    "\n",
    "Using a Tensorflow LSTM layer using the event time steps to the input to predict the Neutrino Direction azimuth and zenith angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:06:01.074512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 15:06:01.170277: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-23 15:06:01.170290: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-23 15:06:01.680862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 15:06:01.680911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 15:06:01.680917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "from sys import getsizeof\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Typing imports\n",
    "from typing import List, Tuple\n",
    "\n",
    "from scripts.utils import seed_it_all, compose_event_df, reduce_mem_usage, convert_bytes_to_gmbkb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IS_TRAINING = True # Whether to train the model\n",
    "SEED=10\n",
    "\n",
    "TIME_LIMIT_HOURS = 1\n",
    "\n",
    "PULSE_AMOUNT = 100 # Amount of pulses to use for features\n",
    "BATCH_SIZE = 32\n",
    "FEATURES = [ 'time', 'charge', 'auxiliary', 'x', 'y', 'z'] # Which features to use as the model input\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"../data\"\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='artifacts/info.log', level=LOG_LEVEL, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_it_all(SEED)\n",
    "# set the seed for the random number generator\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-256.25</td>\n",
       "      <td>-521.0</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_id       x      y      z\n",
       "0          0 -256.25 -521.0  496.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_dtypes = { 'x': 'float16', 'y': 'float16', 'z': 'float16' }\n",
    "sensor_geometry_df = pd.read_csv(f'{DATA_DIR}/sensor_geometry.csv', dtype=sensor_dtypes)\n",
    "sensor_geometry_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70.69 KB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(sensor_geometry_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>first_pulse_index</th>\n",
       "      <th>last_pulse_index</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>5.03125</td>\n",
       "      <td>2.087891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_id  event_id  first_pulse_index  last_pulse_index  azimuth    zenith\n",
       "0         1        24                  0                60  5.03125  2.087891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "meta_dtypes = {'batch_id': 'int16', 'event_id': 'Int64', 'first_pulse_index': 'int32', 'last_pulse_index': 'int32', 'azimuth': 'float16', 'zenith': 'float16'}\n",
    "meta_df = pd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet').astype(meta_dtypes)\n",
    "meta_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch file path 3 Samples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/train/batch_540.parquet',\n",
       " '../data/train/batch_115.parquet',\n",
       " '../data/train/batch_136.parquet']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_directory = f'{DATA_DIR}/{SET}'\n",
    "batch_file_paths = [f'{batch_directory}/{file}' for file in os.listdir(batch_directory) if os.path.isfile(os.path.join(batch_directory, file))]\n",
    "print('First batch file path 3 Samples:')\n",
    "batch_file_paths[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.83 GB'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch_df= pd.read_parquet(batch_file_paths[1])\n",
    "convert_bytes_to_gmbkb(getsizeof(meta_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_data(batch: pd.DataFrame, event_id: int, sequence_length: int, sensor_geometry: pd.DataFrame, meta_data: pd.DataFrame):\n",
    "    # The event dataframe with a list of pulse readings\n",
    "    event_data = batch[batch['event_id'] == event_id]\n",
    "    \n",
    "    merged_df = pd.merge(event_data, sensor_geometry, on='sensor_id', how='left')\n",
    "    \n",
    "    # get the first N pulses with N being the sequence length\n",
    "    sequence = merged_df.head(sequence_length)[FEATURES]\n",
    "    n_missing = 100 - len(sequence)\n",
    "    if n_missing > 0:\n",
    "        df_missing = pd.DataFrame(0, index=np.arange(n_missing), columns=sequence.columns)\n",
    "        sequence = pd.concat([sequence, df_missing])\n",
    "    \n",
    "    # get the target labels \n",
    "    target_labels = meta_data[meta_data['event_id'] == event_id][['azimuth', 'zenith']].values[0] \n",
    "    \n",
    "    # reshape the sequence and target labels to be fed into the model\n",
    "    return np.reshape(sequence.values, (sequence_length, len(FEATURES))), np.reshape(target_labels, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define a generator function\n",
    "def data_generator(\n",
    "    batch_paths:List[str],\n",
    "    sensor_geometry:pd.DataFrame,\n",
    "    meta_data: pd.DataFrame, \n",
    "    sequence_length:int,\n",
    "    batch_size:int=BATCH_SIZE\n",
    "):\n",
    "    \"\"\"Emits a single event training example to be called by the model.fit_generator() method.\n",
    "\n",
    "    Args:\n",
    "        batch_paths (List[str]): A list of paths to the batch files\n",
    "        sensor_geometry_df (pd.DataFrame): The sensor geometry dataframe\n",
    "        meta_df (pd.DataFrame): The dataframe containing the meta data\n",
    "        sequence_length (int): The length of the pulse sequence to use for training\n",
    "\n",
    "    Yields:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    batch_dtypes = {'event_id': 'int32', 'sensor_id': 'int16', 'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'}\n",
    "    \n",
    "   \n",
    "                \n",
    "    for batch_path in batch_paths:\n",
    "        \n",
    "        batch = pd.read_parquet(batch_path).reset_index().astype(batch_dtypes)\n",
    "        \n",
    "        output_batch_x = None\n",
    "        output_batch_y = None\n",
    "        \n",
    "        for event_id in batch['event_id'].unique():\n",
    "            \n",
    "            x_batch, y_batch = get_event_data(batch, event_id, sequence_length , sensor_geometry, meta_data)\n",
    "            \n",
    "            print(len(x_batch),len(x_batch[0]))\n",
    "\n",
    "            if output_batch_x is None and output_batch_y is None:\n",
    "                output_batch_x = tf.constant(x_batch)\n",
    "                output_batch_y = tf.constant(y_batch)\n",
    "                logging.debug('Output_batch initializing') \n",
    "                \n",
    "            else:\n",
    "                output_batch_x = tf.concat([output_batch_x, tf.constant(x_batch)], axis=0)\n",
    "                output_batch_y = tf.concat([output_batch_y, tf.constant(y_batch)], axis=0)\n",
    "                \n",
    "                logging.debug('Output_batch extending: %s', len(output_batch_x))\n",
    "                \n",
    "                    \n",
    "            if len(output_batch_x) == batch_size:\n",
    "                output = output_batch_x[:], output_batch_y[:]\n",
    "                output_batch_x = None\n",
    "                output_batch_y = None\n",
    "                yield output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object\n",
    "data_gen = data_generator(batch_file_paths, sensor_geometry_df, meta_df, sequence_length=PULSE_AMOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n",
      "100 6\n"
     ]
    }
   ],
   "source": [
    "val = next(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, numpy.ndarray)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val[0][0]), type(val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing train input shape of 1 batch (1st index)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 2, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Printing train input shape of 1 batch (1st index)')\n",
    "len(val), len(val[0]), len(val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing label input shape of 1 batch (1st index)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, (2, 100, 1), (2, 100, 1), (2, 100, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Printing label input shape of 1 batch (1st index)')\n",
    "len(val), (len(val[0]), len(val[0][0]), len(val[0][1]) ),(len(val[1]), len(val[1][0]), len(val[1][1]) ),(len(val[2]), len(val[2][0]), len(val[2][1]) ),\n",
    "\n",
    "\n",
    "# len(val[2]), len(val[3]),len(val[4]), len(val[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.01900000e+03,  9.75097656e-01,  1.00000000e+00, -5.26500000e+02,\n",
       "       -1.56015625e+01, -2.63250000e+02])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(val)\n",
    "## first event, train_data_input, first pulse\n",
    "val[0][0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32,input_shape=(PULSE_AMOUNT, len(FEATURES))))\n",
    "model.add(Dense(2, activation='linear')) # set the number of output neurons to 2 and the activation function to linear\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = pd.read_parquet('../data/train/batch_1.parquet').reset_index().astype({'event_id': 'int32', 'sensor_id': 'int16', 'time': 'int32', 'charge': 'float16', 'auxiliary': 'int8'})\n",
    "x_batch, y_batch = get_event_data(batch, 24, PULSE_AMOUNT , sensor_geometry_df, meta_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([[x_batch, y_batch]])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "output_batch = np.array([[x_batch, y_batch]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_batch[:,\u001b[39m1\u001b[39;49m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "output_batch[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x_batch]), len(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n",
      "row 6\n"
     ]
    }
   ],
   "source": [
    "for row in x_batch:\n",
    "    print('row', len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "1 1\n",
      "Epoch 1/10\n",
      " 8/50 [===>..........................] - ETA: 0s - loss: 14.6119 - accuracy: 1.0000 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 500 batches). You may need to use the repeat() function when building your dataset.\n",
      "50/50 [==============================] - 1s 6ms/step - loss: 14.5328 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe016e22670>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model \n",
    "# with input sequence (num_samples, num_timesteps, num_features),\n",
    "# and the azimuth and zenith angle values for each sequence.\n",
    "\n",
    "print(len([x_batch]), len([y_batch]))\n",
    "\n",
    "x_stack = tf.stack([x_batch])\n",
    "y_stack = tf.stack([y_batch])\n",
    "\n",
    "\n",
    "print(len(x_stack), len(y_stack))\n",
    "\n",
    "model.fit(x_stack, y_stack ,steps_per_epoch=50, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate(test_sequences, test_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
