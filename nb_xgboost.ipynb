{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Typing imports\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# parameters\n",
    "BATCH = 1\n",
    "EVENT = 24\n",
    "PULSE_AMOUNT = 200\n",
    "EXCLUDE_AUXILIARY = True\n",
    "IS_TRAINING = True\n",
    "SET = 'train' if IS_TRAINING else 'test'\n",
    "\n",
    "# logging\n",
    "LOG_LEVEL = logging.INFO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "logging.basicConfig(filename='info.log', level=LOG_LEVEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_it_all(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        print(f'Optimizing col {col}')\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "def import_data(file: str):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    function_name = f\"read_{file.split('.')[-1]}\"\n",
    "    function = getattr(pd, function_name)\n",
    "    df = function(file)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def get_event_df(batch_df: dd.DataFrame, sensor_geometry: pd.DataFrame, event_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get a DataFrame for a specific event.\n",
    "\n",
    "    Parameters:\n",
    "    train_batch_df (pandas.DataFrame): The batch DataFrame.\n",
    "    sensor_geometry (pandas.DataFrame): The sensor geometry DataFrame.\n",
    "    event_id (str): The event identifier.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing data for the specified event.\n",
    "    \"\"\"\n",
    "    if EXCLUDE_AUXILIARY:\n",
    "        batch_df = batch_df[~batch_df['auxiliary']].drop(columns=['auxiliary'])\n",
    "    \n",
    "    event_df = batch_df[batch_df['event_id'] == event_id].compute()\n",
    "    # print('event_df out', event_df.head())\n",
    "    event_df = pd.merge(\n",
    "        left=event_df,\n",
    "        right=sensor_geometry,\n",
    "        how='inner',\n",
    "        on='sensor_id'\n",
    "    )\n",
    "    \n",
    "    ## Drop columns that are not needed for prediction\n",
    "    return event_df.drop(columns=['event_id', 'sensor_id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_sphere(x: float, y: float, z: float) -> Tuple[float, float]:\n",
    "    \"\"\"Maps vector cartesian coordinates (x, y, z) from the origin to spherical angles azimuth and zenith.\n",
    "    \n",
    "    See: https://en.wikipedia.org/wiki/Spherical_coordinate_system\n",
    "\n",
    "    Args:\n",
    "        x (float): The x-coordinate of the point.\n",
    "        y (float): The y-coordinate of the point.\n",
    "        z (float): The z-coordinate of the point.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: The azimuth and zenith angles in radians.\n",
    "    \"\"\"\n",
    "    x2y2 = x**2 + y**2\n",
    "#     print('cartesian_to_sphere x**2 + y**2', x2y2)\n",
    "    r = math.sqrt(x2y2 + z**2)\n",
    "#     print('cartesian_to_sphere math.sqrt(x2y2 + z**2)', r)\n",
    "    x_dv_py = 0 if x == 0 else x / math.sqrt(x2y2)\n",
    "    azimuth = math.acos(x_dv_py) * np.sign(y)\n",
    "#     print('math.acos(x / math.sqrt(x2y2)) * np.sign(y)', azimuth)\n",
    "    zenith = math.acos(z / r)\n",
    "#     print('zenith', zenith)\n",
    "    \n",
    "#     print('cartesian_to_sphere takes',x,y,z)\n",
    "#     print('cartesian_to_sphere returns',azimuth, zenith)\n",
    "    return azimuth, zenith\n",
    "\n",
    "\n",
    "def sphere_to_cartesian(azimuth: float, zenith: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"Map spherical coordinates to cartesian coordinates.\n",
    "    see: https://stackoverflow.com/a/10868220/4521646\n",
    "    \n",
    "    Args:\n",
    "        azimuth (float): The azimuth angle in radians.\n",
    "        zenith (float): The zenith angle in radians.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The x, y, z vector cartesian coordinates of the point from the origin.\n",
    "    \"\"\"\n",
    "    x = math.sin(zenith) * math.cos(azimuth)\n",
    "    y = math.sin(zenith) * math.sin(azimuth)\n",
    "    z = math.cos(zenith)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def adjust_sphere(azimuth:float, zenith:float) -> Tuple[float, float]:\n",
    "    \"\"\"Adjust azimuth and zenith to be within [-pi, pi]\n",
    "\n",
    "    Args:\n",
    "        azimuth (float): The azimuth to adjust\n",
    "        zenith (float): The zenith to adjust\n",
    "\n",
    "    Returns:\n",
    "        float: The adjusted azimuth and zenith\n",
    "    \"\"\"\n",
    "    print('adjust_sphere takes',azimuth, zenith)\n",
    "    \n",
    "    if zenith < 0:\n",
    "        zenith += math.pi\n",
    "        azimuth += math.pi\n",
    "    if azimuth < 0:\n",
    "        azimuth += math.pi * 2\n",
    "    azimuth = azimuth % (2 * math.pi)\n",
    "#     print('adjust_sphere returns',azimuth, zenith)\n",
    "    return azimuth, zenith"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_dist_score(az_true:float, zen_true:float, az_pred:float, zen_pred:float):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    The lower the angle, the more similar the two vectors are meaning the score is better.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    \n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    \n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two Cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against numerical instability\n",
    "    # that might otherwise occur from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_geometry = import_data(f'{DATA_DIR}/sensor_geometry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_dfd = dd.read_parquet(f'{DATA_DIR}/{SET}_meta.parquet', \n",
    "    blocksize=64000000 # = 64 Mb chunks\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_dfd = dd.read_parquet(f'{DATA_DIR}/{SET}/batch_1.parquet', \n",
    "        blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "test_batch_dfd.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_vector_shape(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Corrects the shape of the input vector.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe not sized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The newly sized dataframe that has the correct shape and filled with zeros.\n",
    "    \"\"\"\n",
    "    if len(df) < PULSE_AMOUNT:\n",
    "        \n",
    "        blank_df = pd.DataFrame(\n",
    "                index=range(len(df), PULSE_AMOUNT), columns=df.columns\n",
    "            ).fillna(0)\n",
    "        return pd.concat([df, blank_df], ignore_index=True)\n",
    "        \n",
    "    elif len(df) > PULSE_AMOUNT:\n",
    "        return df.head(PULSE_AMOUNT)\n",
    "        \n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_vector(df: pd.DataFrame, event_id: int) -> pd.DataFrame:\n",
    "    \"\"\"Changes the rows of a dataframe to columns\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to be converted that currently has observations in rows\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single observation in columns\n",
    "    \"\"\"\n",
    "    df = make_input_vector_shape(df)\n",
    "    df = df.stack().reset_index()\n",
    "    df['id'] = df['level_0'].astype(str) + '_' + df['level_1']\n",
    "    df = df.drop(columns=['level_0','level_1']).set_index('id')\n",
    "\n",
    "    return df.T.set_index(pd.Index([event_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = get_event_df(test_batch_dfd, sensor_geometry, 24)\n",
    "event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vec = get_input_vector(event_df, 24)\n",
    "input_vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch  1  of  660\n",
      "Processing event 24  of  200000  in batch  1\n",
      "Total time taken so far :  0.07  Minutes\n",
      "Average event time:  4.33  Seconds\n",
      "Remaining Events to process for batch:  199999 . Est time remaining to process:  240.71  Hours\n",
      "Processing event 41  of  200000  in batch  1\n",
      "Total time taken so far :  0.1  Minutes\n",
      "Average event time:  11.6  Seconds\n",
      "Remaining Events to process for batch:  199998 . Est time remaining to process:  644.37  Hours\n",
      "Processing event 59  of  200000  in batch  1\n",
      "Total time taken so far :  0.14  Minutes\n",
      "Average event time:  10.9  Seconds\n",
      "Remaining Events to process for batch:  199997 . Est time remaining to process:  605.74  Hours\n",
      "Processing event 67  of  200000  in batch  1\n",
      "Total time taken so far :  0.17  Minutes\n",
      "Average event time:  8.04  Seconds\n",
      "Remaining Events to process for batch:  199996 . Est time remaining to process:  446.51  Hours\n",
      "Processing event 72  of  200000  in batch  1\n",
      "Total time taken so far :  0.2  Minutes\n",
      "Average event time:  6.05  Seconds\n",
      "Remaining Events to process for batch:  199995 . Est time remaining to process:  335.92  Hours\n",
      "Processing event 77  of  200000  in batch  1\n",
      "Total time taken so far :  0.23  Minutes\n",
      "Average event time:  5.02  Seconds\n",
      "Remaining Events to process for batch:  199994 . Est time remaining to process:  278.99  Hours\n",
      "Processing event 79  of  200000  in batch  1\n",
      "Total time taken so far :  0.27  Minutes\n",
      "Average event time:  4.51  Seconds\n",
      "Remaining Events to process for batch:  199993 . Est time remaining to process:  250.35  Hours\n",
      "Processing event 82  of  200000  in batch  1\n",
      "Total time taken so far :  0.3  Minutes\n",
      "Average event time:  4.21  Seconds\n",
      "Remaining Events to process for batch:  199992 . Est time remaining to process:  233.87  Hours\n",
      "Processing event 121  of  200000  in batch  1\n",
      "Total time taken so far :  0.33  Minutes\n",
      "Average event time:  4.02  Seconds\n",
      "Remaining Events to process for batch:  199991 . Est time remaining to process:  223.05  Hours\n",
      "Processing event 127  of  200000  in batch  1\n",
      "Total time taken so far :  0.36  Minutes\n",
      "Average event time:  3.87  Seconds\n",
      "Remaining Events to process for batch:  199990 . Est time remaining to process:  215.03  Hours\n",
      "Processing event 140  of  200000  in batch  1\n",
      "Total time taken so far :  0.4  Minutes\n",
      "Average event time:  3.76  Seconds\n",
      "Remaining Events to process for batch:  199989 . Est time remaining to process:  209.1  Hours\n",
      "Processing event 221  of  200000  in batch  1\n",
      "Total time taken so far :  0.43  Minutes\n",
      "Average event time:  3.68  Seconds\n",
      "Remaining Events to process for batch:  199988 . Est time remaining to process:  204.47  Hours\n",
      "Processing event 244  of  200000  in batch  1\n",
      "Total time taken so far :  0.46  Minutes\n",
      "Average event time:  3.61  Seconds\n",
      "Remaining Events to process for batch:  199987 . Est time remaining to process:  200.75  Hours\n",
      "Processing event 290  of  200000  in batch  1\n",
      "Total time taken so far :  0.49  Minutes\n",
      "Average event time:  3.56  Seconds\n",
      "Remaining Events to process for batch:  199986 . Est time remaining to process:  197.69  Hours\n",
      "Processing event 325  of  200000  in batch  1\n",
      "Total time taken so far :  0.53  Minutes\n",
      "Average event time:  3.51  Seconds\n",
      "Remaining Events to process for batch:  199985 . Est time remaining to process:  195.05  Hours\n",
      "Processing event 354  of  200000  in batch  1\n",
      "Total time taken so far :  0.56  Minutes\n",
      "Average event time:  3.47  Seconds\n",
      "Remaining Events to process for batch:  199984 . Est time remaining to process:  192.67  Hours\n",
      "Processing event 360  of  200000  in batch  1\n",
      "Total time taken so far :  0.59  Minutes\n",
      "Average event time:  3.43  Seconds\n",
      "Remaining Events to process for batch:  199983 . Est time remaining to process:  190.55  Hours\n",
      "Processing event 402  of  200000  in batch  1\n",
      "Total time taken so far :  0.62  Minutes\n",
      "Average event time:  3.4  Seconds\n",
      "Remaining Events to process for batch:  199982 . Est time remaining to process:  188.74  Hours\n",
      "Processing event 406  of  200000  in batch  1\n",
      "Total time taken so far :  0.65  Minutes\n",
      "Average event time:  3.37  Seconds\n",
      "Remaining Events to process for batch:  199981 . Est time remaining to process:  187.25  Hours\n",
      "Processing event 410  of  200000  in batch  1\n",
      "Total time taken so far :  0.69  Minutes\n",
      "Average event time:  3.35  Seconds\n",
      "Remaining Events to process for batch:  199980 . Est time remaining to process:  185.86  Hours\n",
      "Processing event 422  of  200000  in batch  1\n",
      "Total time taken so far :  0.72  Minutes\n",
      "Average event time:  3.32  Seconds\n",
      "Remaining Events to process for batch:  199979 . Est time remaining to process:  184.6  Hours\n",
      "Processing event 435  of  200000  in batch  1\n",
      "Total time taken so far :  0.75  Minutes\n",
      "Average event time:  3.3  Seconds\n",
      "Remaining Events to process for batch:  199978 . Est time remaining to process:  183.5  Hours\n",
      "Processing event 447  of  200000  in batch  1\n",
      "Total time taken so far :  0.78  Minutes\n",
      "Average event time:  3.28  Seconds\n",
      "Remaining Events to process for batch:  199977 . Est time remaining to process:  182.43  Hours\n",
      "Processing event 448  of  200000  in batch  1\n",
      "Total time taken so far :  0.81  Minutes\n",
      "Average event time:  3.27  Seconds\n",
      "Remaining Events to process for batch:  199976 . Est time remaining to process:  181.46  Hours\n",
      "Processing event 451  of  200000  in batch  1\n",
      "Total time taken so far :  0.85  Minutes\n",
      "Average event time:  3.25  Seconds\n",
      "Remaining Events to process for batch:  199975 . Est time remaining to process:  180.55  Hours\n",
      "Processing event 455  of  200000  in batch  1\n",
      "Total time taken so far :  0.88  Minutes\n",
      "Average event time:  3.24  Seconds\n",
      "Remaining Events to process for batch:  199974 . Est time remaining to process:  179.78  Hours\n",
      "Processing event 472  of  200000  in batch  1\n",
      "Total time taken so far :  0.91  Minutes\n",
      "Average event time:  3.22  Seconds\n",
      "Remaining Events to process for batch:  199973 . Est time remaining to process:  179.09  Hours\n",
      "Processing event 481  of  200000  in batch  1\n",
      "Total time taken so far :  0.94  Minutes\n",
      "Average event time:  3.21  Seconds\n",
      "Remaining Events to process for batch:  199972 . Est time remaining to process:  178.45  Hours\n",
      "Processing event 497  of  200000  in batch  1\n",
      "Total time taken so far :  0.97  Minutes\n",
      "Average event time:  3.2  Seconds\n",
      "Remaining Events to process for batch:  199971 . Est time remaining to process:  177.89  Hours\n",
      "Processing event 538  of  200000  in batch  1\n",
      "Total time taken so far :  1.01  Minutes\n",
      "Average event time:  3.19  Seconds\n",
      "Remaining Events to process for batch:  199970 . Est time remaining to process:  177.34  Hours\n",
      "Processing event 595  of  200000  in batch  1\n",
      "Total time taken so far :  1.04  Minutes\n",
      "Average event time:  3.18  Seconds\n",
      "Remaining Events to process for batch:  199969 . Est time remaining to process:  176.82  Hours\n",
      "Processing event 634  of  200000  in batch  1\n",
      "Total time taken so far :  1.07  Minutes\n",
      "Average event time:  3.17  Seconds\n",
      "Remaining Events to process for batch:  199968 . Est time remaining to process:  176.32  Hours\n",
      "Processing event 663  of  200000  in batch  1\n",
      "Total time taken so far :  1.1  Minutes\n",
      "Average event time:  3.17  Seconds\n",
      "Remaining Events to process for batch:  199967 . Est time remaining to process:  175.85  Hours\n",
      "Processing event 688  of  200000  in batch  1\n",
      "Total time taken so far :  1.13  Minutes\n",
      "Average event time:  3.16  Seconds\n",
      "Remaining Events to process for batch:  199966 . Est time remaining to process:  175.42  Hours\n",
      "Processing event 729  of  200000  in batch  1\n",
      "Total time taken so far :  1.17  Minutes\n",
      "Average event time:  3.15  Seconds\n",
      "Remaining Events to process for batch:  199965 . Est time remaining to process:  175.01  Hours\n",
      "Processing event 730  of  200000  in batch  1\n",
      "Total time taken so far :  1.2  Minutes\n",
      "Average event time:  3.14  Seconds\n",
      "Remaining Events to process for batch:  199964 . Est time remaining to process:  174.62  Hours\n",
      "Processing event 734  of  200000  in batch  1\n",
      "Total time taken so far :  1.23  Minutes\n",
      "Average event time:  3.14  Seconds\n",
      "Remaining Events to process for batch:  199963 . Est time remaining to process:  174.27  Hours\n",
      "Processing event 761  of  200000  in batch  1\n",
      "Total time taken so far :  1.26  Minutes\n",
      "Average event time:  3.13  Seconds\n",
      "Remaining Events to process for batch:  199962 . Est time remaining to process:  174.03  Hours\n",
      "Processing event 769  of  200000  in batch  1\n",
      "Total time taken so far :  1.3  Minutes\n",
      "Average event time:  3.13  Seconds\n",
      "Remaining Events to process for batch:  199961 . Est time remaining to process:  173.82  Hours\n",
      "Processing event 774  of  200000  in batch  1\n",
      "Total time taken so far :  1.33  Minutes\n",
      "Average event time:  3.13  Seconds\n",
      "Remaining Events to process for batch:  199960 . Est time remaining to process:  173.63  Hours\n",
      "Processing event 793  of  200000  in batch  1\n",
      "Total time taken so far :  1.36  Minutes\n",
      "Average event time:  3.12  Seconds\n",
      "Remaining Events to process for batch:  199959 . Est time remaining to process:  173.35  Hours\n",
      "Processing event 794  of  200000  in batch  1\n",
      "Total time taken so far :  1.39  Minutes\n",
      "Average event time:  3.12  Seconds\n",
      "Remaining Events to process for batch:  199958 . Est time remaining to process:  173.11  Hours\n",
      "Processing event 837  of  200000  in batch  1\n",
      "Total time taken so far :  1.43  Minutes\n",
      "Average event time:  3.11  Seconds\n",
      "Remaining Events to process for batch:  199957 . Est time remaining to process:  172.89  Hours\n",
      "Processing event 845  of  200000  in batch  1\n",
      "Total time taken so far :  1.46  Minutes\n",
      "Average event time:  3.11  Seconds\n",
      "Remaining Events to process for batch:  199956 . Est time remaining to process:  172.69  Hours\n",
      "Processing event 904  of  200000  in batch  1\n",
      "Total time taken so far :  1.49  Minutes\n",
      "Average event time:  3.11  Seconds\n",
      "Remaining Events to process for batch:  199955 . Est time remaining to process:  172.51  Hours\n",
      "Processing event 908  of  200000  in batch  1\n",
      "Total time taken so far :  1.52  Minutes\n",
      "Average event time:  3.1  Seconds\n",
      "Remaining Events to process for batch:  199954 . Est time remaining to process:  172.3  Hours\n",
      "Processing event 914  of  200000  in batch  1\n",
      "Total time taken so far :  1.56  Minutes\n",
      "Average event time:  3.1  Seconds\n",
      "Remaining Events to process for batch:  199953 . Est time remaining to process:  172.09  Hours\n",
      "Processing event 932  of  200000  in batch  1\n",
      "Total time taken so far :  1.59  Minutes\n",
      "Average event time:  3.1  Seconds\n",
      "Remaining Events to process for batch:  199952 . Est time remaining to process:  171.94  Hours\n",
      "Processing event 935  of  200000  in batch  1\n",
      "Total time taken so far :  1.62  Minutes\n",
      "Average event time:  3.09  Seconds\n",
      "Remaining Events to process for batch:  199951 . Est time remaining to process:  171.8  Hours\n",
      "Processing event 940  of  200000  in batch  1\n",
      "Total time taken so far :  1.66  Minutes\n",
      "Average event time:  3.09  Seconds\n",
      "Remaining Events to process for batch:  199950 . Est time remaining to process:  171.66  Hours\n",
      "Processing event 959  of  200000  in batch  1\n",
      "Total time taken so far :  1.69  Minutes\n",
      "Average event time:  3.09  Seconds\n",
      "Remaining Events to process for batch:  199949 . Est time remaining to process:  171.48  Hours\n",
      "Processing event 961  of  200000  in batch  1\n",
      "Total time taken so far :  1.72  Minutes\n",
      "Average event time:  3.09  Seconds\n",
      "Remaining Events to process for batch:  199948 . Est time remaining to process:  171.39  Hours\n",
      "Processing event 965  of  200000  in batch  1\n",
      "Total time taken so far :  1.76  Minutes\n",
      "Average event time:  3.08  Seconds\n",
      "Remaining Events to process for batch:  199947 . Est time remaining to process:  171.31  Hours\n",
      "Processing event 984  of  200000  in batch  1\n",
      "Total time taken so far :  1.79  Minutes\n",
      "Average event time:  3.08  Seconds\n",
      "Remaining Events to process for batch:  199946 . Est time remaining to process:  171.16  Hours\n",
      "Processing event 987  of  200000  in batch  1\n",
      "Total time taken so far :  1.82  Minutes\n",
      "Average event time:  3.08  Seconds\n",
      "Remaining Events to process for batch:  199945 . Est time remaining to process:  171.01  Hours\n",
      "Processing event 997  of  200000  in batch  1\n",
      "Total time taken so far :  1.85  Minutes\n",
      "Average event time:  3.08  Seconds\n",
      "Remaining Events to process for batch:  199944 . Est time remaining to process:  170.84  Hours\n",
      "Processing event 1002  of  200000  in batch  1\n",
      "Total time taken so far :  1.88  Minutes\n",
      "Average event time:  3.07  Seconds\n",
      "Remaining Events to process for batch:  199943 . Est time remaining to process:  170.71  Hours\n",
      "Processing event 1007  of  200000  in batch  1\n",
      "Total time taken so far :  1.92  Minutes\n",
      "Average event time:  3.07  Seconds\n",
      "Remaining Events to process for batch:  199942 . Est time remaining to process:  170.6  Hours\n",
      "Processing event 1027  of  200000  in batch  1\n",
      "Total time taken so far :  1.95  Minutes\n",
      "Average event time:  3.07  Seconds\n",
      "Remaining Events to process for batch:  199941 . Est time remaining to process:  170.49  Hours\n",
      "Processing event 1046  of  200000  in batch  1\n",
      "Total time taken so far :  1.98  Minutes\n",
      "Average event time:  3.07  Seconds\n",
      "Remaining Events to process for batch:  199940 . Est time remaining to process:  170.43  Hours\n",
      "Processing event 1091  of  200000  in batch  1\n",
      "Total time taken so far :  2.02  Minutes\n",
      "Average event time:  3.07  Seconds\n",
      "Remaining Events to process for batch:  199939 . Est time remaining to process:  170.29  Hours\n",
      "Processing event 1093  of  200000  in batch  1\n",
      "Total time taken so far :  2.05  Minutes\n",
      "Average event time:  3.06  Seconds\n",
      "Remaining Events to process for batch:  199938 . Est time remaining to process:  170.15  Hours\n",
      "Processing event 1095  of  200000  in batch  1\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "av_batch_time_secs = None\n",
    "av_event_time_secs = None\n",
    "train_start_time = time.time()\n",
    "\n",
    "batches = meta_dfd['batch_id'].unique().compute().values\n",
    "\n",
    "for i, batch_id in enumerate(batches):\n",
    "    \n",
    "    print('Processing batch ', batch_id, ' of ', len(batches))\n",
    "    \n",
    "    # The batch dataframe to be populated with events\n",
    "    batch_df = None\n",
    "    \n",
    "    batch_dfd = dd.read_parquet(f'{DATA_DIR}/{SET}/batch_{batch_id}.parquet', \n",
    "        blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "    \n",
    "    # get the current date and time\n",
    "    now = datetime.datetime.now()\n",
    "    # create a date string with the format day-month-year-hour:minute\n",
    "    date_string = now.strftime('%d-%m-%Y-%H:%M')\n",
    "    # define the file path\n",
    "    file_path = f'artifacts/{SET}/{date_string}/batch_{batch_id}.csv'\n",
    "    parent_dir = os.path.dirname(file_path)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "        \n",
    "    # Loop through unique event IDs\n",
    "    events = batch_dfd['event_id'].unique().compute().values\n",
    "    \n",
    "    for j, event_id in enumerate(events):\n",
    "        \n",
    "        print('Processing event', event_id, ' of ', len(events), ' in batch ', batch_id)\n",
    "        event_df = get_event_df(batch_dfd, sensor_geometry, event_id)\n",
    "        \n",
    "        input_vec =  get_input_vector(event_df, event_id)\n",
    "        \n",
    "                # check if a DataFrame exists\n",
    "        if batch_df is not None:\n",
    "            \n",
    "            batch_df = pd.concat([ batch_df, input_vec])\n",
    "            input_vec.to_csv(file_path, mode='a', header=False, index=True)\n",
    "        else:\n",
    "            # handle the case where the DataFrame does not exist\n",
    "            batch_df = input_vec\n",
    "            batch_df.to_csv(file_path, index=True, index_label='event_id')\n",
    "         \n",
    "        \n",
    "        # Time tracking\n",
    "        current_time = time.time() - train_start_time\n",
    "        mins = current_time / 60\n",
    "        print(\"Total time taken so far : \", round(mins, 2), \" Minutes\")\n",
    "        av_event_time_secs = current_time if av_event_time_secs is None else (av_event_time_secs + current_time) / j + 1\n",
    "        print('Average event time: ', round(av_event_time_secs, 2), \" Seconds\")\n",
    "        remaining_events = len(events) - j - 1\n",
    "        remaining_event_minutes = (av_event_time_secs * remaining_events)\n",
    "        print(\n",
    "            'Remaining Events to process for batch: ',\n",
    "              remaining_events, '. Est time remaining to process: ', \n",
    "              round(remaining_event_minutes / 60 / 60, 2), \" Hours\"\n",
    "            )\n",
    "    \n",
    "    if batch_df is not None:\n",
    "       \n",
    "        file_path = f'artifacts/{SET}/{date_string}/{batch_id}.npy'\n",
    "        # create the parent directories if they don't exist\n",
    "        parent_dir = os.path.dirname(file_path)\n",
    "        \n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "        batch_df.to_numpy(file_path)\n",
    "        \n",
    "        current_time = time.time() - train_start_time\n",
    "        av_batch_time_secs = current_time if av_batch_time_secs is None else (av_batch_time_secs + current_time) / i + 1\n",
    "        print('Average batch time: ', round(av_batch_time_secs / 60, 2), \" Minutes\")\n",
    "        remaining_batches = len(events) - i - 1\n",
    "        remaining_batch_hours = (av_batch_time_secs * remaining_batches) / 60 / 60\n",
    "        print(\n",
    "            'Remaining Events to process for batch: ',\n",
    "              remaining_batches, '. Est time remaining to process: ', \n",
    "              round(remaining_batch_hours, 2), \" Hours\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['event_id', 'azimuth', 'zenith'])\n",
    "\n",
    "for batch_id in meta_dfd['batch_id'].unique().compute().values:\n",
    "    \n",
    "    print('Processing batch ', batch_id)\n",
    "    \n",
    "    batch_dfd = dd.read_parquet(f'{DATA_DIR}/{SET}/batch_{batch_id}.parquet', \n",
    "        blocksize=64000000 # = 64 Mb chunks,\n",
    "    ).reset_index()\n",
    "    \n",
    "        \n",
    "    # Loop through unique event IDs\n",
    "    for event_id in batch_dfd['event_id'].unique().compute().values:\n",
    "        \n",
    "        print('Processing event', event_id, ' in batch ', batch_id)\n",
    "        event_df = get_event_df(batch_id,sensor_geometry, event_id)\n",
    "        \n",
    "        input_vec =  get_input_vector(event_df, event_id)\n",
    "        \n",
    "        submission = pd.concat([ new_row, submission.loc[:]]) \n",
    "        current_time = time.time() \n",
    "        print(\"Total time taken so far : \", current_time - start_time, \"seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = meta_dfd['batch_id'].unique().compute().values # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_hours = total_time / 60 / 60\n",
    "print(\"Total time taken: \", round(total_hours,2), \"Hours\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAG_IC_NEU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad40088d582bffe2fc05846b5516a111df7b25e3d4e8e50a24f706fb2c5c2959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
